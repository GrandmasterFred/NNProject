{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    " # this file will try to trian the two models defined here. And determine which is the best performing one"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import Module, Sequential, LeakyReLU, Conv2d, BatchNorm2d, AvgPool2d, MaxPool2d, AdaptiveAvgPool2d, Linear, Dropout\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "# this is adapted from https://github.com/Moeo3/GoogLeNet-Inception-V3-pytorch/blob/master/googlenet_v3.py#L58 with its size modified to suit the CIFAR10 dataset instead of the origianl ImageNet dataset.\n",
    "\n",
    "''' comments on models\n",
    "the orignal model has\n",
    "3 conv_bn\n",
    "1 pool\n",
    "2 conv_bn\n",
    "1 pool\n",
    "3x inception a\n",
    "1x inception b\n",
    "4x inception c\n",
    "1x inception d\n",
    "2x inception e\n",
    "1 conv_bn\n",
    "1 adaptive pool 2d\n",
    "\n",
    "dropout\n",
    "flatten\n",
    "\n",
    "fully connected layer\n",
    "\n",
    "===================================\n",
    "for our model, we are gonna just makeit smaller so that it trains faster, also cifar10 does not have the 1000 classes in imagenet lol\n",
    "'''\n",
    "\n",
    "class InceptionModel1(Module):\n",
    "    def __init__(self, channels_in, class_num = 10):\n",
    "        super(InceptionModel1, self).__init__()\n",
    "        # remember, i must be able to extract the feature maps of each of the convolution layers, and as such, i must design my network around that as well\n",
    "\n",
    "        # if this one is false, it will return feature maps. This would be found at the return funtion in the forward function\n",
    "        self.PCA = False\n",
    "        # input is N, 3, 32, 32\n",
    "        self.layer1 = Sequential(\n",
    "            Conv2d_BN(channels_in = channels_in, channels_out= 32, kernel_size=3, stride=2, padding=1),\n",
    "            MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1)\n",
    "             )\n",
    "        # N, 32, 16, 16\n",
    "        self.layer2 = Sequential(\n",
    "            Conv2d_BN(channels_in = 32, channels_out= 64, kernel_size=3, stride=2, padding=1),\n",
    "            MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1)\n",
    "        )\n",
    "        # N, 64, 8, 8\n",
    "\n",
    "        # going into the inception layers\n",
    "        # note that each of the components inside inception will ALWAYS retain the same width and height, and all the channels are concatenated together thats all\n",
    "        self.incep1 = InceptionA(64, 16)\n",
    "        # N, 240, 8, 8\n",
    "        self.incep2 = InceptionB(240)\n",
    "        # N, 720, 8, 8\n",
    "        # inception 3 barely fits CIFAR10, i think this will be the last layer\n",
    "        self.incep3 = InceptionC(720, 128)\n",
    "        # N, 768, 8, 8\n",
    "        self.incep4 = InceptionD(768)\n",
    "        # N, 1280, 8, 8\n",
    "        self.incep5 = InceptionE(1280)\n",
    "        # N, 2048, 8, 8\n",
    "\n",
    "        # going into the output layer now, last conv layer and then flattening it\n",
    "        self.out = Sequential(\n",
    "            # lowering the number of channels\n",
    "            Conv2d_BN(channels_in = 2048, channels_out= 1024, kernel_size=1, stride=1, padding=0),\n",
    "            AdaptiveAvgPool2d(1),\n",
    "            Dropout(0.5)\n",
    "        )\n",
    "        # N, 1024, 1, 1\n",
    "\n",
    "        # this one will then output it based on the number of classes, based on softmax i guess at this point\n",
    "        self.fc = Linear(1024, class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        fmap1 = x.clone()\n",
    "        x = self.layer2(x)\n",
    "        fmap2 = x.clone()\n",
    "        x = self.incep1(x)\n",
    "        fmap3 = x.clone()\n",
    "        x = self.incep2(x)\n",
    "        fmap4 = x.clone()\n",
    "        x = self.incep3(x)\n",
    "        fmap5 = x.clone()\n",
    "        x = self.incep4(x)\n",
    "        fmap6 = x.clone()\n",
    "        x = self.incep5(x)\n",
    "        fmap7 = x.clone()\n",
    "        x = self.out(x)\n",
    "        fmap8 = x.clone()\n",
    "        # this one is to flatten, and retaining the batch size\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        fmap9 = x.clone()\n",
    "\n",
    "        if self.PCA:\n",
    "            return x, (fmap1, fmap2, fmap3, fmap3, fmap4, fmap5, fmap6, fmap7, fmap8, fmap9)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "class InceptionModel2(Module):\n",
    "    def __init__(self, channels_in, class_num = 10):\n",
    "        super(InceptionModel2, self).__init__()\n",
    "        # remember, i must be able to extract the feature maps of each of the convolution layers, and as such, i must design my network around that as well\n",
    "\n",
    "        # if this one is false, it will return feature maps. This would be found at the return funtion in the forward function\n",
    "        self.PCA = False\n",
    "        # input is N, 3, 32, 32\n",
    "        self.layer1 = Sequential(\n",
    "            Conv2d_BN(channels_in = channels_in, channels_out= 32, kernel_size=3, stride=2, padding=1),\n",
    "            MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1)\n",
    "        )\n",
    "        # N, 32, 16, 16\n",
    "        self.layer2 = Sequential(\n",
    "            Conv2d_BN(channels_in = 32, channels_out= 64, kernel_size=3, stride=2, padding=1),\n",
    "            MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1)\n",
    "        )\n",
    "        # N, 64, 8, 8\n",
    "\n",
    "        # going into the inception layers\n",
    "        # note that each of the components inside inception will ALWAYS retain the same width and height, and all the channels are concatenated together thats all\n",
    "        self.incep1 = InceptionA(64, 16)\n",
    "        # N, 240, 8, 8\n",
    "        self.incep2 = InceptionB(240)\n",
    "        # N, 720, 8, 8\n",
    "        # inception 3 barely fits CIFAR10, i think this will be the last layer\n",
    "        self.incep3 = InceptionC(720, 128)\n",
    "        # N, 768, 8, 8\n",
    "\n",
    "        # going into the output layer now, last conv layer and then flattening it\n",
    "        self.out = Sequential(\n",
    "            # lowering the number of channels\n",
    "            Conv2d_BN(channels_in = 768, channels_out= 320, kernel_size=1, stride=1, padding=0),\n",
    "            AdaptiveAvgPool2d(1),\n",
    "            Dropout(0.5)\n",
    "        )\n",
    "        # N, 1024, 1, 1\n",
    "\n",
    "        # this one will then output it based on the number of classes, based on softmax i guess at this point\n",
    "        self.fc = Linear(320, class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        fmap1 = x.clone()\n",
    "        x = self.layer2(x)\n",
    "        fmap2 = x.clone()\n",
    "        x = self.incep1(x)\n",
    "        fmap3 = x.clone()\n",
    "        x = self.incep2(x)\n",
    "        fmap4 = x.clone()\n",
    "        x = self.incep3(x)\n",
    "        fmap5 = x.clone()\n",
    "        x = self.out(x)\n",
    "        fmap8 = x.clone()\n",
    "        # this one is to flatten, and retaining the batch size\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        fmap9 = x.clone()\n",
    "\n",
    "        if self.PCA:\n",
    "            return x, (fmap1, fmap2, fmap3, fmap3, fmap4, fmap5, fmap8, fmap9)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "# this is the intermediate modules\n",
    "class Conv2d_BN(Module):\n",
    "    def __init__(self, channels_in, channels_out, kernel_size, padding, stride=1, acti=LeakyReLU(0.2, inplace=True)):\n",
    "        super(Conv2d_BN, self).__init__()\n",
    "        self.conv2d_bn = Sequential(\n",
    "            Conv2d(channels_in, channels_out, kernel_size, stride, padding, bias=False),\n",
    "            BatchNorm2d(channels_out),\n",
    "            acti\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv2d_bn(x)\n",
    "\n",
    "class InceptionA(Module):\n",
    "    def __init__(self, channels_in, pool_channels):\n",
    "        super(InceptionA, self).__init__()\n",
    "        self.branch1x1 = Conv2d_BN(channels_in, 64, 1, stride=1, padding=0)  # 64 channels\n",
    "        self.branch5x5 = Sequential(\n",
    "            Conv2d_BN(channels_in, 48, 1, stride=1, padding=0),\n",
    "            Conv2d_BN(48, 64, 5, stride=1, padding=2)\n",
    "        )  # 64 channels\n",
    "        self.branch3x3dbl = Sequential(\n",
    "            Conv2d_BN(channels_in, 64, 1, stride=1, padding=0),\n",
    "            Conv2d_BN(64, 96, 3, stride=1, padding=1),\n",
    "            Conv2d_BN(96, 96, 3, stride=1, padding=1)\n",
    "        )  # 96 channels\n",
    "        self.branch_pool = Sequential(\n",
    "            AvgPool2d(3, stride=1, padding=1),\n",
    "            Conv2d_BN(channels_in, pool_channels, 1, stride=1, padding=0)\n",
    "        )  # pool_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [self.branch1x1(x), self.branch5x5(x), self.branch3x3dbl(x), self.branch_pool(x)]\n",
    "        # 64 + 64 + 96 + pool_channels\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "class InceptionB(Module):\n",
    "    def __init__(self, channels_in):\n",
    "        super(InceptionB, self).__init__()\n",
    "        self.branch3x3 = Conv2d_BN(channels_in, 384, 3, stride=2, padding=1)  # 384 channels\n",
    "        self.branch3x3dbl = Sequential(\n",
    "            Conv2d_BN(channels_in, 64, 1, padding=0),\n",
    "            Conv2d_BN(64, 96, 3, padding=1),\n",
    "            Conv2d_BN(96, 96, 3, stride=2, padding=1)\n",
    "        )  # 96 channels\n",
    "        self.branch_pool = MaxPool2d(3, stride=2, padding=1)  # channels_in\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [self.branch3x3(x), self.branch3x3dbl(x), self.branch_pool(x)]\n",
    "        # 384 + 96 + channels_in\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "class InceptionC(Module):\n",
    "    def __init__(self, channels_in, channels_7x7):\n",
    "        super(InceptionC, self).__init__()\n",
    "        self.branch1x1 = Conv2d_BN(channels_in, 192, 1, stride=1, padding=0)  # 192 channels\n",
    "        self.branch7x7 = Sequential(\n",
    "            Conv2d_BN(channels_in, channels_7x7, 1, stride=1, padding=0),\n",
    "            Conv2d_BN(channels_7x7, channels_7x7, (1, 7), stride=1, padding=(0, 3)),\n",
    "            Conv2d_BN(channels_7x7, 192, (7, 1), stride=1, padding=(3, 0))\n",
    "        )  # 192 channels\n",
    "        self.branch7x7dbl = Sequential(\n",
    "            Conv2d_BN(channels_in, channels_7x7, 1, stride=1, padding=0),\n",
    "            Conv2d_BN(channels_7x7, channels_7x7, (7, 1), stride=1, padding=(3, 0)),\n",
    "            Conv2d_BN(channels_7x7, channels_7x7, (1, 7), stride=1, padding=(0, 3)),\n",
    "            Conv2d_BN(channels_7x7, channels_7x7, (7, 1), stride=1, padding=(3, 0)),\n",
    "            Conv2d_BN(channels_7x7, 192, (1, 7), stride=1, padding=(0, 3))\n",
    "        )  # 192 channels\n",
    "        self.branch_pool = Sequential(\n",
    "            AvgPool2d(3, stride=1, padding=1),\n",
    "            Conv2d_BN(channels_in, 192, 1, stride=1, padding=0)\n",
    "        )  # 192 channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [self.branch1x1(x), self.branch7x7(x), self.branch7x7dbl(x), self.branch_pool(x)]\n",
    "        # 192 + 192 + 192 + 192 = 768 channels\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "class InceptionD(Module):\n",
    "    def __init__(self, channels_in):\n",
    "        super(InceptionD, self).__init__()\n",
    "        self.branch3x3 = Sequential(\n",
    "            Conv2d_BN(channels_in, 192, 1, stride=1, padding=0),\n",
    "            Conv2d_BN(192, 320, 3, stride=2, padding=1)\n",
    "        )  # 320 channels\n",
    "        self.branch7x7x3 = Sequential(\n",
    "            Conv2d_BN(channels_in, 192, 1, stride=1, padding=0),\n",
    "            Conv2d_BN(192, 192, (1, 7), stride=1, padding=(0, 3)),\n",
    "            Conv2d_BN(192, 192, (7, 1), stride=1, padding=(3, 0)),\n",
    "            Conv2d_BN(192, 192, 3, stride=2, padding=1)\n",
    "        )  # 192 chnnels\n",
    "        self.branch_pool = MaxPool2d(3, stride=2, padding=1)  # channels_in\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [self.branch3x3(x), self.branch7x7x3(x), self.branch_pool(x)]\n",
    "        # 320 + 192 + channels_in\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "class InceptionE(Module):\n",
    "    def __init__(self, channels_in):\n",
    "        super(InceptionE, self).__init__()\n",
    "        self.branch1x1 = Conv2d_BN(channels_in, 320, 1, stride=1, padding=0)  # 320 channels\n",
    "\n",
    "        self.branch3x3_1 = Conv2d_BN(channels_in, 384, 1, stride=1, padding=0)\n",
    "        self.branch3x3_2a = Conv2d_BN(384, 384, (1, 3), stride=1, padding=(0, 1))\n",
    "        self.branch3x3_2b = Conv2d_BN(384, 384, (3, 1), stride=1, padding=(1, 0))\n",
    "        # 768 channels\n",
    "\n",
    "        self.branch3x3dbl_1 = Sequential(\n",
    "            Conv2d_BN(channels_in, 448, 1, stride=1, padding=0),\n",
    "            Conv2d_BN(448, 384, 3, stride=1, padding=1)\n",
    "        )\n",
    "        self.branch3x3dbl_2a = Conv2d_BN(384, 384, (1, 3), stride=1, padding=(0, 1))\n",
    "        self.branch3x3dbl_2b = Conv2d_BN(384, 384, (3, 1), stride=1, padding=(1, 0))\n",
    "        # 768 channels\n",
    "\n",
    "        self.branch_pool = Sequential(\n",
    "            AvgPool2d(3, stride=1, padding=1),\n",
    "            Conv2d_BN(channels_in, 192, 1, stride=1, padding=0)\n",
    "        )  # 192 channels\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = torch.cat([self.branch3x3_2a(branch3x3), self.branch3x3_2b(branch3x3)], 1)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = torch.cat([self.branch3x3dbl_2a(branch3x3dbl), self.branch3x3dbl_2b(branch3x3dbl)], 1)\n",
    "\n",
    "        branch_pool = self.branch_pool(x)\n",
    "\n",
    "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
    "        # 320 + 768 + 768 + 192 = 2048 channels\n",
    "        return torch.cat(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Number of training examples: 40000\n",
      "Number of validation examples: 10000\n",
      "Number of test examples: 10000\n"
     ]
    }
   ],
   "source": [
    "STUDENTID = 567     # this will be used for random states\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import logging\n",
    "from utils import *\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL Image to PyTorch Tensor\n",
    "    # this normalization is industry standard, it seems\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # Normalize the data\n",
    "])\n",
    "\n",
    "\n",
    "# define the label transformations, from int64 to float32\n",
    "transform_label = transforms.Compose([\n",
    "    #transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Download and load CIFAR-100 datasets\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True, target_transform=transform_label)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform, download=True, target_transform=transform_label)\n",
    "\n",
    "\n",
    "# Calculate the sizes for train, validation, and test sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "valid_size = len(train_dataset) - train_size\n",
    "\n",
    "# Split the train dataset into train and validation sets using a random seed\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [train_size, valid_size], generator=torch.Generator().manual_seed(STUDENTID))\n",
    "\n",
    "# Create data loaders for training, validation, and test sets\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(valid_dataset))\n",
    "print(\"Number of test examples:\", len(test_dataset))\n",
    "\n",
    "# creating a model that automatically runs the forward function i guess, since it is easier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils import *\n",
    "from customModels import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Number of training examples: 40000\n",
      "Number of validation examples: 10000\n",
      "Number of test examples: 10000\n",
      "training starts here\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [3], line 87\u001B[0m\n\u001B[0;32m     84\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m     86\u001B[0m \u001B[38;5;66;03m# training and saving model to dictionary\u001B[39;00m\n\u001B[1;32m---> 87\u001B[0m outputDict \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margDict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     89\u001B[0m \u001B[38;5;66;03m# loading the best model, and then sending it off to testing\u001B[39;00m\n\u001B[0;32m     90\u001B[0m model \u001B[38;5;241m=\u001B[39m load_model_from_file(model, argDict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutputName\u001B[39m\u001B[38;5;124m'\u001B[39m], argDict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutputName\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[1;32mD:\\gitprojects\\NNProject\\utils.py:245\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, argDict, givenDataloader, evalDataloader, testDataloader)\u001B[0m\n\u001B[0;32m    242\u001B[0m data, label \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mto(device), label\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m    244\u001B[0m \u001B[38;5;66;03m# this will be the training loop\u001B[39;00m\n\u001B[1;32m--> 245\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    247\u001B[0m loss \u001B[38;5;241m=\u001B[39m argDict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcriterion\u001B[39m\u001B[38;5;124m'\u001B[39m](outputs, label)\n\u001B[0;32m    249\u001B[0m \u001B[38;5;66;03m# backward pass and optimization\u001B[39;00m\n",
      "File \u001B[1;32mD:\\gitprojects\\FinerObjectDetectionVENV\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn [1], line 144\u001B[0m, in \u001B[0;36mInceptionModel2.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m--> 144\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    145\u001B[0m     fmap1 \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mclone()\n\u001B[0;32m    146\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer2(x)\n",
      "File \u001B[1;32mD:\\gitprojects\\FinerObjectDetectionVENV\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\gitprojects\\FinerObjectDetectionVENV\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    202\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    203\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 204\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    205\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32mD:\\gitprojects\\FinerObjectDetectionVENV\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn [1], line 178\u001B[0m, in \u001B[0;36mConv2d_BN.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m--> 178\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d_bn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\gitprojects\\FinerObjectDetectionVENV\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\gitprojects\\FinerObjectDetectionVENV\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    202\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    203\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 204\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    205\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32mD:\\gitprojects\\FinerObjectDetectionVENV\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\gitprojects\\FinerObjectDetectionVENV\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\gitprojects\\FinerObjectDetectionVENV\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    457\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    458\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = InceptionModel2(channels_in=3, class_num=10)\n",
    "modelName = 'InceptionModel2_crop_flip'\n",
    "\n",
    "# this transformation sequence must be modified to work with the random stuff\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL Image to PyTorch Tensor\n",
    "    # this normalization is industry standard, it seems\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomCrop(size=28, padding=2, pad_if_needed=True, padding_mode=\"edge\"),\n",
    "    transforms.Resize(32),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # Normalize the data\n",
    "])\n",
    "\n",
    "argDict = {\n",
    "    'lr': 0.001,\n",
    "    'maxEpoch': 500,\n",
    "    'idleEpoch': 25,\n",
    "    'outputName': modelName,\n",
    "    'optimizer': optim.SGD(model.parameters(), lr=0.001),\n",
    "    'criterion': nn.CrossEntropyLoss()\n",
    "}\n",
    "\n",
    "STUDENTID = 567     # this will be used for random states\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import logging\n",
    "from utils import *\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "# define the label transformations, from int64 to float32\n",
    "transform_label = transforms.Compose([\n",
    "    #transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Download and load CIFAR-100 datasets\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True, target_transform=transform_label)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform, download=True, target_transform=transform_label)\n",
    "\n",
    "\n",
    "# Calculate the sizes for train, validation, and test sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "valid_size = len(train_dataset) - train_size\n",
    "\n",
    "# Split the train dataset into train and validation sets using a random seed\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [train_size, valid_size], generator=torch.Generator().manual_seed(STUDENTID))\n",
    "\n",
    "# Create data loaders for training, validation, and test sets\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(valid_dataset))\n",
    "print(\"Number of test examples:\", len(test_dataset))\n",
    "\n",
    "# creating a model that automatically runs the forward function i guess, since it is easier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils import *\n",
    "from customModels import *\n",
    "\n",
    "# setting up the logger\n",
    "loggerName = modelName + '.log'\n",
    "loggerName = os.path.join(argDict['outputName'], loggerName)\n",
    "logger = MyLogger(loggerName)\n",
    "argDict['logger'] = logger\n",
    "\n",
    "# just to initilalize the files\n",
    "logger.log('training starts here')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# training and saving model to dictionary\n",
    "outputDict = train(model, argDict, train_loader, val_loader, test_loader)\n",
    "\n",
    "# loading the best model, and then sending it off to testing\n",
    "model = load_model_from_file(model, argDict['outputName'], argDict['outputName'])\n",
    "\n",
    "test_accuracy = test(model, argDict, test_loader)\n",
    "tempString = 'testing accuracy of ' + argDict['outputName'] + \" is: \" + str(test_accuracy)\n",
    "logger.log(tempString)\n",
    "\n",
    "argDict['test_accuracy'] = str(test_accuracy)\n",
    "\n",
    "# timing the thing as well\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "argDict['time_taken'] = execution_time\n",
    "save_dict_to_file(outputDict, argDict['outputName'], argDict['outputName'])\n",
    "\n",
    "del model\n",
    "del argDict\n",
    "\n",
    "# Define the folder you want to zip and the output zip file name\n",
    "folder_to_zip = modelName\n",
    "output_zip_file = modelName + \".zip\"\n",
    "\n",
    "# Use shutil.make_archive to create the zip file\n",
    "shutil.make_archive(output_zip_file, 'zip', folder_to_zip)\n",
    "\n",
    "os.rename(output_zip_file + '.zip', output_zip_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}