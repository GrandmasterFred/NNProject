{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    " # this file will try to trian the two models defined here. And determine which is the best performing one"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import Module, Sequential, LeakyReLU, Conv2d, BatchNorm2d, AvgPool2d, MaxPool2d, AdaptiveAvgPool2d, Linear, Dropout\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "# this is adapted from https://github.com/Moeo3/GoogLeNet-Inception-V3-pytorch/blob/master/googlenet_v3.py#L58 with its size modified to suit the CIFAR10 dataset instead of the origianl ImageNet dataset.\n",
    "\n",
    "''' comments on models\n",
    "the orignal model has\n",
    "3 conv_bn\n",
    "1 pool\n",
    "2 conv_bn\n",
    "1 pool\n",
    "3x inception a\n",
    "1x inception b\n",
    "4x inception c\n",
    "1x inception d\n",
    "2x inception e\n",
    "1 conv_bn\n",
    "1 adaptive pool 2d\n",
    "\n",
    "dropout\n",
    "flatten\n",
    "\n",
    "fully connected layer\n",
    "\n",
    "===================================\n",
    "for our model, we are gonna just makeit smaller so that it trains faster, also cifar10 does not have the 1000 classes in imagenet lol\n",
    "'''\n",
    "\n",
    "class InceptionModel1(Module):\n",
    "    def __init__(self, channels_in, class_num = 10):\n",
    "        super(InceptionModel1, self).__init__()\n",
    "        # remember, i must be able to extract the feature maps of each of the convolution layers, and as such, i must design my network around that as well\n",
    "\n",
    "        # if this one is false, it will return feature maps. This would be found at the return funtion in the forward function\n",
    "        self.PCA = False\n",
    "        # input is N, 3, 32, 32\n",
    "        self.layer1 = Sequential(\n",
    "            Conv2d_BN(channels_in = channels_in, channels_out= 32, kernel_size=3, stride=2, padding=1),\n",
    "            MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1)\n",
    "             )\n",
    "        # N, 32, 16, 16\n",
    "        self.layer2 = Sequential(\n",
    "            Conv2d_BN(channels_in = 32, channels_out= 64, kernel_size=3, stride=2, padding=1),\n",
    "            MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1)\n",
    "        )\n",
    "        # N, 64, 8, 8\n",
    "\n",
    "        # going into the inception layers\n",
    "        # note that each of the components inside inception will ALWAYS retain the same width and height, and all the channels are concatenated together thats all\n",
    "        self.incep1 = InceptionA(64, 16)\n",
    "        # N, 240, 8, 8\n",
    "        self.incep2 = InceptionB(240)\n",
    "        # N, 720, 8, 8\n",
    "        # inception 3 barely fits CIFAR10, i think this will be the last layer\n",
    "        self.incep3 = InceptionC(720, 128)\n",
    "        # N, 768, 8, 8\n",
    "        self.incep4 = InceptionD(768)\n",
    "        # N, 1280, 8, 8\n",
    "        self.incep5 = InceptionE(1280)\n",
    "        # N, 2048, 8, 8\n",
    "\n",
    "        # going into the output layer now, last conv layer and then flattening it\n",
    "        self.out = Sequential(\n",
    "            # lowering the number of channels\n",
    "            Conv2d_BN(channels_in = 2048, channels_out= 1024, kernel_size=1, stride=1, padding=0),\n",
    "            AdaptiveAvgPool2d(1),\n",
    "            Dropout(0.5)\n",
    "        )\n",
    "        # N, 1024, 1, 1\n",
    "\n",
    "        # this one will then output it based on the number of classes, based on softmax i guess at this point\n",
    "        self.fc = Linear(1024, class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        fmap1 = x.clone()\n",
    "        x = self.layer2(x)\n",
    "        fmap2 = x.clone()\n",
    "        x = self.incep1(x)\n",
    "        fmap3 = x.clone()\n",
    "        x = self.incep2(x)\n",
    "        fmap4 = x.clone()\n",
    "        x = self.incep3(x)\n",
    "        fmap5 = x.clone()\n",
    "        x = self.incep4(x)\n",
    "        fmap6 = x.clone()\n",
    "        x = self.incep5(x)\n",
    "        fmap7 = x.clone()\n",
    "        x = self.out(x)\n",
    "        fmap8 = x.clone()\n",
    "        # this one is to flatten, and retaining the batch size\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        fmap9 = x.clone()\n",
    "\n",
    "        if self.PCA:\n",
    "            return x, (fmap1, fmap2, fmap3, fmap3, fmap4, fmap5, fmap6, fmap7, fmap8, fmap9)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "class InceptionModel2(Module):\n",
    "    def __init__(self, channels_in, class_num = 10):\n",
    "        super(InceptionModel2, self).__init__()\n",
    "        # remember, i must be able to extract the feature maps of each of the convolution layers, and as such, i must design my network around that as well\n",
    "\n",
    "        # if this one is false, it will return feature maps. This would be found at the return funtion in the forward function\n",
    "        self.PCA = False\n",
    "        # input is N, 3, 32, 32\n",
    "        self.layer1 = Sequential(\n",
    "            Conv2d_BN(channels_in = channels_in, channels_out= 32, kernel_size=3, stride=2, padding=1),\n",
    "            MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1)\n",
    "        )\n",
    "        # N, 32, 16, 16\n",
    "        self.layer2 = Sequential(\n",
    "            Conv2d_BN(channels_in = 32, channels_out= 64, kernel_size=3, stride=2, padding=1),\n",
    "            MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1)\n",
    "        )\n",
    "        # N, 64, 8, 8\n",
    "\n",
    "        # going into the inception layers\n",
    "        # note that each of the components inside inception will ALWAYS retain the same width and height, and all the channels are concatenated together thats all\n",
    "        self.incep1 = InceptionA(64, 16)\n",
    "        # N, 240, 8, 8\n",
    "        self.incep2 = InceptionB(240)\n",
    "        # N, 720, 8, 8\n",
    "        # inception 3 barely fits CIFAR10, i think this will be the last layer\n",
    "        self.incep3 = InceptionC(720, 128)\n",
    "        # N, 768, 8, 8\n",
    "\n",
    "        # going into the output layer now, last conv layer and then flattening it\n",
    "        self.out = Sequential(\n",
    "            # lowering the number of channels\n",
    "            Conv2d_BN(channels_in = 768, channels_out= 320, kernel_size=1, stride=1, padding=0),\n",
    "            AdaptiveAvgPool2d(1),\n",
    "            Dropout(0.5)\n",
    "        )\n",
    "        # N, 1024, 1, 1\n",
    "\n",
    "        # this one will then output it based on the number of classes, based on softmax i guess at this point\n",
    "        self.fc = Linear(320, class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        fmap1 = x.clone()\n",
    "        x = self.layer2(x)\n",
    "        fmap2 = x.clone()\n",
    "        x = self.incep1(x)\n",
    "        fmap3 = x.clone()\n",
    "        x = self.incep2(x)\n",
    "        fmap4 = x.clone()\n",
    "        x = self.incep3(x)\n",
    "        fmap5 = x.clone()\n",
    "        x = self.out(x)\n",
    "        fmap8 = x.clone()\n",
    "        # this one is to flatten, and retaining the batch size\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        fmap9 = x.clone()\n",
    "\n",
    "        if self.PCA:\n",
    "            return x, (fmap1, fmap2, fmap3, fmap3, fmap4, fmap5, fmap8, fmap9)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "class InceptionModel3(Module):\n",
    "    def __init__(self, channels_in, class_num = 10):\n",
    "        super(InceptionModel3, self).__init__()\n",
    "        # remember, i must be able to extract the feature maps of each of the convolution layers, and as such, i must design my network around that as well\n",
    "\n",
    "        # if this one is false, it will return feature maps. This would be found at the return funtion in the forward function\n",
    "        self.PCA = False\n",
    "        # input is N, 3, 32, 32\n",
    "        self.layer1 = Sequential(\n",
    "            Conv2d_BN(channels_in = channels_in, channels_out= 32, kernel_size=3, stride=2, padding=1),\n",
    "            MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1)\n",
    "        )\n",
    "        # N, 32, 16, 16\n",
    "        self.layer2 = Sequential(\n",
    "            Conv2d_BN(channels_in = 32, channels_out= 64, kernel_size=3, stride=2, padding=1),\n",
    "            MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1)\n",
    "        )\n",
    "        # N, 64, 8, 8\n",
    "\n",
    "        # going into the inception layers\n",
    "        # note that each of the components inside inception will ALWAYS retain the same width and height, and all the channels are concatenated together thats all\n",
    "        self.incep1 = InceptionA(64, 16)\n",
    "        # N, 240, 8, 8\n",
    "        self.incep2 = InceptionB(240)\n",
    "        # N, 720, 8, 8\n",
    "\n",
    "        # going into the output layer now, last conv layer and then flattening it\n",
    "        self.out = Sequential(\n",
    "            # lowering the number of channels\n",
    "            Conv2d_BN(channels_in = 720, channels_out= 320, kernel_size=1, stride=1, padding=0),\n",
    "            AdaptiveAvgPool2d(1),\n",
    "            Dropout(0.5)\n",
    "        )\n",
    "        # N, 1024, 1, 1\n",
    "\n",
    "        # this one will then output it based on the number of classes, based on softmax i guess at this point\n",
    "        self.fc = Linear(320, class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        fmap1 = x.clone()\n",
    "        x = self.layer2(x)\n",
    "        fmap2 = x.clone()\n",
    "        x = self.incep1(x)\n",
    "        fmap3 = x.clone()\n",
    "        x = self.incep2(x)\n",
    "        fmap4 = x.clone()\n",
    "        x = self.out(x)\n",
    "        fmap8 = x.clone()\n",
    "        # this one is to flatten, and retaining the batch size\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        fmap9 = x.clone()\n",
    "\n",
    "        if self.PCA:\n",
    "            return x, (fmap1, fmap2, fmap3, fmap3, fmap4, fmap8, fmap9)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "# this is the intermediate modules\n",
    "class Conv2d_BN(Module):\n",
    "    def __init__(self, channels_in, channels_out, kernel_size, padding, stride=1, acti=LeakyReLU(0.2, inplace=True)):\n",
    "        super(Conv2d_BN, self).__init__()\n",
    "        self.conv2d_bn = Sequential(\n",
    "            Conv2d(channels_in, channels_out, kernel_size, stride, padding, bias=False),\n",
    "            BatchNorm2d(channels_out),\n",
    "            acti\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv2d_bn(x)\n",
    "\n",
    "class InceptionA(Module):\n",
    "    def __init__(self, channels_in, pool_channels):\n",
    "        super(InceptionA, self).__init__()\n",
    "        self.branch1x1 = Conv2d_BN(channels_in, 64, 1, stride=1, padding=0)  # 64 channels\n",
    "        self.branch5x5 = Sequential(\n",
    "            Conv2d_BN(channels_in, 48, 1, stride=1, padding=0),\n",
    "            Conv2d_BN(48, 64, 5, stride=1, padding=2)\n",
    "        )  # 64 channels\n",
    "        self.branch3x3dbl = Sequential(\n",
    "            Conv2d_BN(channels_in, 64, 1, stride=1, padding=0),\n",
    "            Conv2d_BN(64, 96, 3, stride=1, padding=1),\n",
    "            Conv2d_BN(96, 96, 3, stride=1, padding=1)\n",
    "        )  # 96 channels\n",
    "        self.branch_pool = Sequential(\n",
    "            AvgPool2d(3, stride=1, padding=1),\n",
    "            Conv2d_BN(channels_in, pool_channels, 1, stride=1, padding=0)\n",
    "        )  # pool_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [self.branch1x1(x), self.branch5x5(x), self.branch3x3dbl(x), self.branch_pool(x)]\n",
    "        # 64 + 64 + 96 + pool_channels\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "class InceptionB(Module):\n",
    "    def __init__(self, channels_in):\n",
    "        super(InceptionB, self).__init__()\n",
    "        self.branch3x3 = Conv2d_BN(channels_in, 384, 3, stride=2, padding=1)  # 384 channels\n",
    "        self.branch3x3dbl = Sequential(\n",
    "            Conv2d_BN(channels_in, 64, 1, padding=0),\n",
    "            Conv2d_BN(64, 96, 3, padding=1),\n",
    "            Conv2d_BN(96, 96, 3, stride=2, padding=1)\n",
    "        )  # 96 channels\n",
    "        self.branch_pool = MaxPool2d(3, stride=2, padding=1)  # channels_in\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [self.branch3x3(x), self.branch3x3dbl(x), self.branch_pool(x)]\n",
    "        # 384 + 96 + channels_in\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "class InceptionC(Module):\n",
    "    def __init__(self, channels_in, channels_7x7):\n",
    "        super(InceptionC, self).__init__()\n",
    "        self.branch1x1 = Conv2d_BN(channels_in, 192, 1, stride=1, padding=0)  # 192 channels\n",
    "        self.branch7x7 = Sequential(\n",
    "            Conv2d_BN(channels_in, channels_7x7, 1, stride=1, padding=0),\n",
    "            Conv2d_BN(channels_7x7, channels_7x7, (1, 7), stride=1, padding=(0, 3)),\n",
    "            Conv2d_BN(channels_7x7, 192, (7, 1), stride=1, padding=(3, 0))\n",
    "        )  # 192 channels\n",
    "        self.branch7x7dbl = Sequential(\n",
    "            Conv2d_BN(channels_in, channels_7x7, 1, stride=1, padding=0),\n",
    "            Conv2d_BN(channels_7x7, channels_7x7, (7, 1), stride=1, padding=(3, 0)),\n",
    "            Conv2d_BN(channels_7x7, channels_7x7, (1, 7), stride=1, padding=(0, 3)),\n",
    "            Conv2d_BN(channels_7x7, channels_7x7, (7, 1), stride=1, padding=(3, 0)),\n",
    "            Conv2d_BN(channels_7x7, 192, (1, 7), stride=1, padding=(0, 3))\n",
    "        )  # 192 channels\n",
    "        self.branch_pool = Sequential(\n",
    "            AvgPool2d(3, stride=1, padding=1),\n",
    "            Conv2d_BN(channels_in, 192, 1, stride=1, padding=0)\n",
    "        )  # 192 channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [self.branch1x1(x), self.branch7x7(x), self.branch7x7dbl(x), self.branch_pool(x)]\n",
    "        # 192 + 192 + 192 + 192 = 768 channels\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "class InceptionD(Module):\n",
    "    def __init__(self, channels_in):\n",
    "        super(InceptionD, self).__init__()\n",
    "        self.branch3x3 = Sequential(\n",
    "            Conv2d_BN(channels_in, 192, 1, stride=1, padding=0),\n",
    "            Conv2d_BN(192, 320, 3, stride=2, padding=1)\n",
    "        )  # 320 channels\n",
    "        self.branch7x7x3 = Sequential(\n",
    "            Conv2d_BN(channels_in, 192, 1, stride=1, padding=0),\n",
    "            Conv2d_BN(192, 192, (1, 7), stride=1, padding=(0, 3)),\n",
    "            Conv2d_BN(192, 192, (7, 1), stride=1, padding=(3, 0)),\n",
    "            Conv2d_BN(192, 192, 3, stride=2, padding=1)\n",
    "        )  # 192 chnnels\n",
    "        self.branch_pool = MaxPool2d(3, stride=2, padding=1)  # channels_in\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [self.branch3x3(x), self.branch7x7x3(x), self.branch_pool(x)]\n",
    "        # 320 + 192 + channels_in\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "class InceptionE(Module):\n",
    "    def __init__(self, channels_in):\n",
    "        super(InceptionE, self).__init__()\n",
    "        self.branch1x1 = Conv2d_BN(channels_in, 320, 1, stride=1, padding=0)  # 320 channels\n",
    "\n",
    "        self.branch3x3_1 = Conv2d_BN(channels_in, 384, 1, stride=1, padding=0)\n",
    "        self.branch3x3_2a = Conv2d_BN(384, 384, (1, 3), stride=1, padding=(0, 1))\n",
    "        self.branch3x3_2b = Conv2d_BN(384, 384, (3, 1), stride=1, padding=(1, 0))\n",
    "        # 768 channels\n",
    "\n",
    "        self.branch3x3dbl_1 = Sequential(\n",
    "            Conv2d_BN(channels_in, 448, 1, stride=1, padding=0),\n",
    "            Conv2d_BN(448, 384, 3, stride=1, padding=1)\n",
    "        )\n",
    "        self.branch3x3dbl_2a = Conv2d_BN(384, 384, (1, 3), stride=1, padding=(0, 1))\n",
    "        self.branch3x3dbl_2b = Conv2d_BN(384, 384, (3, 1), stride=1, padding=(1, 0))\n",
    "        # 768 channels\n",
    "\n",
    "        self.branch_pool = Sequential(\n",
    "            AvgPool2d(3, stride=1, padding=1),\n",
    "            Conv2d_BN(channels_in, 192, 1, stride=1, padding=0)\n",
    "        )  # 192 channels\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = torch.cat([self.branch3x3_2a(branch3x3), self.branch3x3_2b(branch3x3)], 1)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = torch.cat([self.branch3x3dbl_2a(branch3x3dbl), self.branch3x3dbl_2b(branch3x3dbl)], 1)\n",
    "\n",
    "        branch_pool = self.branch_pool(x)\n",
    "\n",
    "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
    "        # 320 + 768 + 768 + 192 = 2048 channels\n",
    "        return torch.cat(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Number of training examples: 40000\n",
      "Number of validation examples: 10000\n",
      "Number of test examples: 10000\n"
     ]
    }
   ],
   "source": [
    "STUDENTID = 567     # this will be used for random states\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import logging\n",
    "from utils import *\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL Image to PyTorch Tensor\n",
    "    # this normalization is industry standard, it seems\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # Normalize the data\n",
    "])\n",
    "\n",
    "\n",
    "# define the label transformations, from int64 to float32\n",
    "transform_label = transforms.Compose([\n",
    "    #transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Download and load CIFAR-100 datasets\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True, target_transform=transform_label)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform, download=True, target_transform=transform_label)\n",
    "\n",
    "\n",
    "# Calculate the sizes for train, validation, and test sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "valid_size = len(train_dataset) - train_size\n",
    "\n",
    "# Split the train dataset into train and validation sets using a random seed\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [train_size, valid_size], generator=torch.Generator().manual_seed(STUDENTID))\n",
    "\n",
    "# Create data loaders for training, validation, and test sets\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(valid_dataset))\n",
    "print(\"Number of test examples:\", len(test_dataset))\n",
    "\n",
    "# creating a model that automatically runs the forward function i guess, since it is easier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils import *\n",
    "from customModels import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "InceptionModel3                               [16, 10]                  --\n",
      "├─Sequential: 1-1                             [16, 32, 7, 7]            --\n",
      "│    └─Conv2d_BN: 2-1                         [16, 32, 16, 16]          --\n",
      "│    │    └─Sequential: 3-1                   [16, 32, 16, 16]          928\n",
      "├─Sequential: 1-28                            --                        (recursive)\n",
      "│    └─Conv2d_BN: 2-28                        --                        (recursive)\n",
      "│    │    └─Sequential: 3-27                  --                        (recursive)\n",
      "├─Sequential: 1-3                             --                        (recursive)\n",
      "│    └─MaxPool2d: 2-3                         [16, 32, 7, 7]            --\n",
      "├─Sequential: 1-4                             [16, 64, 1, 1]            --\n",
      "│    └─Conv2d_BN: 2-4                         [16, 64, 4, 4]            --\n",
      "│    │    └─Sequential: 3-3                   [16, 64, 4, 4]            18,560\n",
      "├─Sequential: 1-28                            --                        (recursive)\n",
      "│    └─Conv2d_BN: 2-28                        --                        (recursive)\n",
      "│    │    └─Sequential: 3-27                  --                        (recursive)\n",
      "├─Sequential: 1-6                             --                        (recursive)\n",
      "│    └─MaxPool2d: 2-6                         [16, 64, 1, 1]            --\n",
      "├─InceptionA: 1-7                             [16, 240, 1, 1]           224,000\n",
      "│    └─Conv2d_BN: 2-7                         [16, 64, 1, 1]            --\n",
      "│    │    └─Sequential: 3-5                   [16, 64, 1, 1]            4,224\n",
      "├─Sequential: 1-28                            --                        (recursive)\n",
      "│    └─Conv2d_BN: 2-28                        --                        (recursive)\n",
      "│    │    └─Sequential: 3-27                  --                        (recursive)\n",
      "├─InceptionA: 1-19                            --                        (recursive)\n",
      "│    └─Sequential: 2-9                        [16, 64, 1, 1]            76,928\n",
      "│    │    └─Conv2d_BN: 3-7                    [16, 48, 1, 1]            3,168\n",
      "├─Sequential: 1-28                            --                        (recursive)\n",
      "│    └─Conv2d_BN: 2-28                        --                        (recursive)\n",
      "│    │    └─Sequential: 3-27                  --                        (recursive)\n",
      "├─InceptionA: 1-19                            --                        (recursive)\n",
      "│    └─Sequential: 2-11                       --                        (recursive)\n",
      "│    │    └─Conv2d_BN: 3-9                    [16, 64, 1, 1]            76,928\n",
      "├─Sequential: 1-28                            --                        (recursive)\n",
      "│    └─Conv2d_BN: 2-28                        --                        (recursive)\n",
      "│    │    └─Sequential: 3-27                  --                        (recursive)\n",
      "├─InceptionA: 1-19                            --                        (recursive)\n",
      "│    └─Sequential: 2-13                       [16, 96, 1, 1]            138,624\n",
      "│    │    └─Conv2d_BN: 3-11                   [16, 64, 1, 1]            4,224\n",
      "├─Sequential: 1-28                            --                        (recursive)\n",
      "│    └─Conv2d_BN: 2-28                        --                        (recursive)\n",
      "│    │    └─Sequential: 3-27                  --                        (recursive)\n",
      "├─InceptionA: 1-19                            --                        (recursive)\n",
      "│    └─Sequential: 2-17                       --                        (recursive)\n",
      "│    │    └─Conv2d_BN: 3-13                   [16, 96, 1, 1]            55,488\n",
      "├─Sequential: 1-28                            --                        (recursive)\n",
      "│    └─Conv2d_BN: 2-28                        --                        (recursive)\n",
      "│    │    └─Sequential: 3-27                  --                        (recursive)\n",
      "├─InceptionA: 1-19                            --                        (recursive)\n",
      "│    └─Sequential: 2-17                       --                        (recursive)\n",
      "│    │    └─Conv2d_BN: 3-15                   [16, 96, 1, 1]            83,136\n",
      "├─Sequential: 1-28                            --                        (recursive)\n",
      "│    └─Conv2d_BN: 2-28                        --                        (recursive)\n",
      "│    │    └─Sequential: 3-27                  --                        (recursive)\n",
      "├─InceptionA: 1-19                            --                        (recursive)\n",
      "│    └─Sequential: 2-19                       [16, 16, 1, 1]            --\n",
      "│    │    └─AvgPool2d: 3-17                   [16, 64, 1, 1]            --\n",
      "│    │    └─Conv2d_BN: 3-18                   [16, 16, 1, 1]            1,056\n",
      "├─Sequential: 1-28                            --                        (recursive)\n",
      "│    └─Conv2d_BN: 2-28                        --                        (recursive)\n",
      "│    │    └─Sequential: 3-27                  --                        (recursive)\n",
      "├─InceptionB: 1-21                            [16, 720, 1, 1]           154,112\n",
      "│    └─Conv2d_BN: 2-21                        [16, 384, 1, 1]           --\n",
      "│    │    └─Sequential: 3-20                  [16, 384, 1, 1]           830,208\n",
      "├─Sequential: 1-28                            --                        (recursive)\n",
      "│    └─Conv2d_BN: 2-28                        --                        (recursive)\n",
      "│    │    └─Sequential: 3-27                  --                        (recursive)\n",
      "├─InceptionB: 1-29                            --                        (recursive)\n",
      "│    └─Sequential: 2-23                       [16, 96, 1, 1]            138,624\n",
      "│    │    └─Conv2d_BN: 3-22                   [16, 64, 1, 1]            15,488\n",
      "├─Sequential: 1-28                            --                        (recursive)\n",
      "│    └─Conv2d_BN: 2-28                        --                        (recursive)\n",
      "│    │    └─Sequential: 3-27                  --                        (recursive)\n",
      "├─InceptionB: 1-29                            --                        (recursive)\n",
      "│    └─Sequential: 2-27                       --                        (recursive)\n",
      "│    │    └─Conv2d_BN: 3-24                   [16, 96, 1, 1]            55,488\n",
      "├─Sequential: 1-28                            --                        (recursive)\n",
      "│    └─Conv2d_BN: 2-28                        --                        (recursive)\n",
      "│    │    └─Sequential: 3-27                  --                        (recursive)\n",
      "├─InceptionB: 1-29                            --                        (recursive)\n",
      "│    └─Sequential: 2-27                       --                        (recursive)\n",
      "│    │    └─Conv2d_BN: 3-26                   [16, 96, 1, 1]            83,136\n",
      "├─Sequential: 1-28                            --                        (recursive)\n",
      "│    └─Conv2d_BN: 2-28                        --                        (recursive)\n",
      "│    │    └─Sequential: 3-27                  --                        (recursive)\n",
      "├─InceptionB: 1-29                            --                        (recursive)\n",
      "│    └─MaxPool2d: 2-29                        [16, 240, 1, 1]           --\n",
      "├─Sequential: 1-30                            [16, 320, 1, 1]           --\n",
      "│    └─Conv2d_BN: 2-30                        [16, 320, 1, 1]           --\n",
      "│    │    └─Sequential: 3-28                  [16, 320, 1, 1]           231,040\n",
      "│    └─AdaptiveAvgPool2d: 2-31                [16, 320, 1, 1]           --\n",
      "│    └─Dropout: 2-32                          [16, 320, 1, 1]           --\n",
      "├─Linear: 1-31                                [16, 10]                  3,210\n",
      "===============================================================================================\n",
      "Total params: 2,198,570\n",
      "Trainable params: 2,198,570\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 31.41\n",
      "===============================================================================================\n",
      "Input size (MB): 0.20\n",
      "Forward/backward pass size (MB): 2.72\n",
      "Params size (MB): 5.87\n",
      "Estimated Total Size (MB): 8.78\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = InceptionModel3(channels_in=3, class_num=10)\n",
    "from torchinfo import summary\n",
    "\n",
    "print(summary(model, (16, 3, 32, 32)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = InceptionModel3(channels_in=3, class_num=10)\n",
    "modelName = 'InceptionModel3_base'\n",
    "\n",
    "# this transformation sequence must be modified to work with the random stuff\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL Image to PyTorch Tensor\n",
    "    # this normalization is industry standard, it seems\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # Normalize the data\n",
    "])\n",
    "\n",
    "argDict = {\n",
    "    'lr': 0.001,\n",
    "    'maxEpoch': 500,\n",
    "    'idleEpoch': 25,\n",
    "    'outputName': modelName,\n",
    "    'optimizer': optim.SGD(model.parameters(), lr=0.001),\n",
    "    'criterion': nn.CrossEntropyLoss()\n",
    "}\n",
    "\n",
    "STUDENTID = 567     # this will be used for random states\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import logging\n",
    "from utils import *\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "# define the label transformations, from int64 to float32\n",
    "transform_label = transforms.Compose([\n",
    "    #transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Download and load CIFAR-100 datasets\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True, target_transform=transform_label)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform, download=True, target_transform=transform_label)\n",
    "\n",
    "\n",
    "# Calculate the sizes for train, validation, and test sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "valid_size = len(train_dataset) - train_size\n",
    "\n",
    "# Split the train dataset into train and validation sets using a random seed\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [train_size, valid_size], generator=torch.Generator().manual_seed(STUDENTID))\n",
    "\n",
    "# Create data loaders for training, validation, and test sets\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(valid_dataset))\n",
    "print(\"Number of test examples:\", len(test_dataset))\n",
    "\n",
    "# creating a model that automatically runs the forward function i guess, since it is easier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils import *\n",
    "from customModels import *\n",
    "\n",
    "# setting up the logger\n",
    "loggerName = modelName + '.log'\n",
    "loggerName = os.path.join(argDict['outputName'], loggerName)\n",
    "logger = MyLogger(loggerName)\n",
    "argDict['logger'] = logger\n",
    "\n",
    "# just to initilalize the files\n",
    "logger.log('training starts here')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# training and saving model to dictionary\n",
    "outputDict = train(model, argDict, train_loader, val_loader, test_loader)\n",
    "\n",
    "# loading the best model, and then sending it off to testing\n",
    "model = load_model_from_file(model, argDict['outputName'], argDict['outputName'])\n",
    "\n",
    "test_accuracy = test(model, argDict, test_loader)\n",
    "tempString = 'testing accuracy of ' + argDict['outputName'] + \" is: \" + str(test_accuracy)\n",
    "logger.log(tempString)\n",
    "\n",
    "argDict['test_accuracy'] = str(test_accuracy)\n",
    "\n",
    "# timing the thing as well\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "argDict['time_taken'] = execution_time\n",
    "save_dict_to_file(outputDict, argDict['outputName'], argDict['outputName'])\n",
    "\n",
    "del model\n",
    "del argDict\n",
    "\n",
    "# Define the folder you want to zip and the output zip file name\n",
    "folder_to_zip = modelName\n",
    "output_zip_file = modelName + \".zip\"\n",
    "\n",
    "# Use shutil.make_archive to create the zip file\n",
    "shutil.make_archive(output_zip_file, 'zip', folder_to_zip)\n",
    "\n",
    "os.rename(output_zip_file + '.zip', output_zip_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Number of training examples: 40000\n",
      "Number of validation examples: 10000\n",
      "Number of test examples: 10000\n",
      "training starts here\n",
      "currently at epoch 0 train accuracy: tensor(0.2651, device='cuda:0') loss of: 1.9998745420455932 eval accuracy: tensor(0.3115, device='cuda:0')\n",
      "currently at epoch 1 train accuracy: tensor(0.3394, device='cuda:0') loss of: 1.7993052368164062 eval accuracy: tensor(0.3529, device='cuda:0')\n",
      "currently at epoch 2 train accuracy: tensor(0.3741, device='cuda:0') loss of: 1.717935894393921 eval accuracy: tensor(0.3789, device='cuda:0')\n",
      "currently at epoch 3 train accuracy: tensor(0.3968, device='cuda:0') loss of: 1.662701590538025 eval accuracy: tensor(0.3962, device='cuda:0')\n",
      "currently at epoch 4 train accuracy: tensor(0.4097, device='cuda:0') loss of: 1.6310093156814576 eval accuracy: tensor(0.4178, device='cuda:0')\n",
      "currently at epoch 5 train accuracy: tensor(0.4251, device='cuda:0') loss of: 1.5941158555984498 eval accuracy: tensor(0.4273, device='cuda:0')\n",
      "currently at epoch 6 train accuracy: tensor(0.4347, device='cuda:0') loss of: 1.56828348197937 eval accuracy: tensor(0.4334, device='cuda:0')\n",
      "currently at epoch 7 train accuracy: tensor(0.4444, device='cuda:0') loss of: 1.5422979846954346 eval accuracy: tensor(0.4397, device='cuda:0')\n",
      "currently at epoch 8 train accuracy: tensor(0.4515, device='cuda:0') loss of: 1.5263979194641113 eval accuracy: tensor(0.4471, device='cuda:0')\n",
      "currently at epoch 9 train accuracy: tensor(0.4573, device='cuda:0') loss of: 1.5072081996917726 eval accuracy: tensor(0.4555, device='cuda:0')\n",
      "currently at epoch 10 train accuracy: tensor(0.4681, device='cuda:0') loss of: 1.4876599643707276 eval accuracy: tensor(0.4638, device='cuda:0')\n",
      "currently at epoch 11 train accuracy: tensor(0.4728, device='cuda:0') loss of: 1.4777766382217408 eval accuracy: tensor(0.4702, device='cuda:0')\n",
      "currently at epoch 12 train accuracy: tensor(0.4806, device='cuda:0') loss of: 1.4560468297958373 eval accuracy: tensor(0.4763, device='cuda:0')\n",
      "currently at epoch 13 train accuracy: tensor(0.4834, device='cuda:0') loss of: 1.441337685775757 eval accuracy: tensor(0.4818, device='cuda:0')\n",
      "currently at epoch 14 train accuracy: tensor(0.4880, device='cuda:0') loss of: 1.4325884519577026 eval accuracy: tensor(0.4760, device='cuda:0')\n",
      "currently at epoch 15 train accuracy: tensor(0.4961, device='cuda:0') loss of: 1.423848509979248 eval accuracy: tensor(0.4852, device='cuda:0')\n",
      "currently at epoch 16 train accuracy: tensor(0.4934, device='cuda:0') loss of: 1.409788145828247 eval accuracy: tensor(0.4870, device='cuda:0')\n",
      "currently at epoch 17 train accuracy: tensor(0.5008, device='cuda:0') loss of: 1.39842856464386 eval accuracy: tensor(0.4845, device='cuda:0')\n",
      "currently at epoch 18 train accuracy: tensor(0.5034, device='cuda:0') loss of: 1.3945741498947144 eval accuracy: tensor(0.4967, device='cuda:0')\n",
      "currently at epoch 19 train accuracy: tensor(0.5069, device='cuda:0') loss of: 1.3780635318756103 eval accuracy: tensor(0.4979, device='cuda:0')\n",
      "currently at epoch 20 train accuracy: tensor(0.5130, device='cuda:0') loss of: 1.3730646354675293 eval accuracy: tensor(0.4988, device='cuda:0')\n",
      "currently at epoch 21 train accuracy: tensor(0.5137, device='cuda:0') loss of: 1.359676807975769 eval accuracy: tensor(0.5013, device='cuda:0')\n",
      "currently at epoch 22 train accuracy: tensor(0.5181, device='cuda:0') loss of: 1.3534368538856507 eval accuracy: tensor(0.5050, device='cuda:0')\n",
      "currently at epoch 23 train accuracy: tensor(0.5218, device='cuda:0') loss of: 1.343530216884613 eval accuracy: tensor(0.5118, device='cuda:0')\n",
      "currently at epoch 24 train accuracy: tensor(0.5202, device='cuda:0') loss of: 1.341760419178009 eval accuracy: tensor(0.5101, device='cuda:0')\n",
      "currently at epoch 25 train accuracy: tensor(0.5286, device='cuda:0') loss of: 1.3286122625350951 eval accuracy: tensor(0.5122, device='cuda:0')\n",
      "currently at epoch 26 train accuracy: tensor(0.5268, device='cuda:0') loss of: 1.3276397436141967 eval accuracy: tensor(0.5119, device='cuda:0')\n",
      "currently at epoch 27 train accuracy: tensor(0.5321, device='cuda:0') loss of: 1.3152404434204101 eval accuracy: tensor(0.5197, device='cuda:0')\n",
      "currently at epoch 28 train accuracy: tensor(0.5320, device='cuda:0') loss of: 1.31092291431427 eval accuracy: tensor(0.5133, device='cuda:0')\n",
      "currently at epoch 29 train accuracy: tensor(0.5342, device='cuda:0') loss of: 1.306978461265564 eval accuracy: tensor(0.5184, device='cuda:0')\n",
      "currently at epoch 30 train accuracy: tensor(0.5391, device='cuda:0') loss of: 1.297217311668396 eval accuracy: tensor(0.5221, device='cuda:0')\n",
      "currently at epoch 31 train accuracy: tensor(0.5372, device='cuda:0') loss of: 1.3008822226524352 eval accuracy: tensor(0.5183, device='cuda:0')\n",
      "currently at epoch 32 train accuracy: tensor(0.5370, device='cuda:0') loss of: 1.2943276355743407 eval accuracy: tensor(0.5257, device='cuda:0')\n",
      "currently at epoch 33 train accuracy: tensor(0.5448, device='cuda:0') loss of: 1.2800998761177063 eval accuracy: tensor(0.5301, device='cuda:0')\n",
      "currently at epoch 34 train accuracy: tensor(0.5481, device='cuda:0') loss of: 1.2736071590423583 eval accuracy: tensor(0.5339, device='cuda:0')\n",
      "currently at epoch 35 train accuracy: tensor(0.5502, device='cuda:0') loss of: 1.26917309923172 eval accuracy: tensor(0.5314, device='cuda:0')\n",
      "currently at epoch 36 train accuracy: tensor(0.5527, device='cuda:0') loss of: 1.2606229588508606 eval accuracy: tensor(0.5300, device='cuda:0')\n",
      "currently at epoch 37 train accuracy: tensor(0.5531, device='cuda:0') loss of: 1.2639394684791565 eval accuracy: tensor(0.5335, device='cuda:0')\n",
      "currently at epoch 38 train accuracy: tensor(0.5509, device='cuda:0') loss of: 1.2614400946617126 eval accuracy: tensor(0.5318, device='cuda:0')\n",
      "currently at epoch 39 train accuracy: tensor(0.5573, device='cuda:0') loss of: 1.2519257514953612 eval accuracy: tensor(0.5337, device='cuda:0')\n",
      "currently at epoch 40 train accuracy: tensor(0.5574, device='cuda:0') loss of: 1.2434386944770812 eval accuracy: tensor(0.5344, device='cuda:0')\n",
      "currently at epoch 41 train accuracy: tensor(0.5574, device='cuda:0') loss of: 1.2409757066726685 eval accuracy: tensor(0.5423, device='cuda:0')\n",
      "currently at epoch 42 train accuracy: tensor(0.5610, device='cuda:0') loss of: 1.2381840347290038 eval accuracy: tensor(0.5377, device='cuda:0')\n",
      "currently at epoch 43 train accuracy: tensor(0.5638, device='cuda:0') loss of: 1.2325784996032716 eval accuracy: tensor(0.5398, device='cuda:0')\n",
      "currently at epoch 44 train accuracy: tensor(0.5624, device='cuda:0') loss of: 1.2292469745635985 eval accuracy: tensor(0.5430, device='cuda:0')\n",
      "currently at epoch 45 train accuracy: tensor(0.5654, device='cuda:0') loss of: 1.224244429397583 eval accuracy: tensor(0.5474, device='cuda:0')\n",
      "currently at epoch 46 train accuracy: tensor(0.5645, device='cuda:0') loss of: 1.223101613998413 eval accuracy: tensor(0.5382, device='cuda:0')\n",
      "currently at epoch 47 train accuracy: tensor(0.5703, device='cuda:0') loss of: 1.2130797462463379 eval accuracy: tensor(0.5421, device='cuda:0')\n",
      "currently at epoch 48 train accuracy: tensor(0.5729, device='cuda:0') loss of: 1.2106925859451294 eval accuracy: tensor(0.5453, device='cuda:0')\n",
      "currently at epoch 49 train accuracy: tensor(0.5727, device='cuda:0') loss of: 1.2002675238609315 eval accuracy: tensor(0.5445, device='cuda:0')\n",
      "currently at epoch 50 train accuracy: tensor(0.5754, device='cuda:0') loss of: 1.1988100034713745 eval accuracy: tensor(0.5410, device='cuda:0')\n",
      "currently at epoch 51 train accuracy: tensor(0.5712, device='cuda:0') loss of: 1.2062107792854309 eval accuracy: tensor(0.5492, device='cuda:0')\n",
      "currently at epoch 52 train accuracy: tensor(0.5787, device='cuda:0') loss of: 1.1960972414016724 eval accuracy: tensor(0.5484, device='cuda:0')\n",
      "currently at epoch 53 train accuracy: tensor(0.5764, device='cuda:0') loss of: 1.192246886920929 eval accuracy: tensor(0.5500, device='cuda:0')\n",
      "currently at epoch 54 train accuracy: tensor(0.5814, device='cuda:0') loss of: 1.186138887310028 eval accuracy: tensor(0.5484, device='cuda:0')\n",
      "currently at epoch 55 train accuracy: tensor(0.5830, device='cuda:0') loss of: 1.1801271352767944 eval accuracy: tensor(0.5488, device='cuda:0')\n",
      "currently at epoch 56 train accuracy: tensor(0.5847, device='cuda:0') loss of: 1.1771552154541016 eval accuracy: tensor(0.5560, device='cuda:0')\n",
      "currently at epoch 57 train accuracy: tensor(0.5830, device='cuda:0') loss of: 1.1786248126983643 eval accuracy: tensor(0.5571, device='cuda:0')\n",
      "currently at epoch 58 train accuracy: tensor(0.5904, device='cuda:0') loss of: 1.1620865231513977 eval accuracy: tensor(0.5556, device='cuda:0')\n",
      "currently at epoch 59 train accuracy: tensor(0.5860, device='cuda:0') loss of: 1.1670586382865906 eval accuracy: tensor(0.5541, device='cuda:0')\n",
      "currently at epoch 60 train accuracy: tensor(0.5892, device='cuda:0') loss of: 1.1621977417945861 eval accuracy: tensor(0.5601, device='cuda:0')\n",
      "currently at epoch 61 train accuracy: tensor(0.5886, device='cuda:0') loss of: 1.1597551299095155 eval accuracy: tensor(0.5613, device='cuda:0')\n",
      "currently at epoch 62 train accuracy: tensor(0.5907, device='cuda:0') loss of: 1.1587155923843384 eval accuracy: tensor(0.5654, device='cuda:0')\n",
      "currently at epoch 63 train accuracy: tensor(0.5910, device='cuda:0') loss of: 1.156607444667816 eval accuracy: tensor(0.5576, device='cuda:0')\n",
      "currently at epoch 64 train accuracy: tensor(0.5957, device='cuda:0') loss of: 1.1410930846214296 eval accuracy: tensor(0.5595, device='cuda:0')\n",
      "currently at epoch 65 train accuracy: tensor(0.5940, device='cuda:0') loss of: 1.150190754699707 eval accuracy: tensor(0.5603, device='cuda:0')\n",
      "currently at epoch 66 train accuracy: tensor(0.5934, device='cuda:0') loss of: 1.1446735354423523 eval accuracy: tensor(0.5678, device='cuda:0')\n",
      "currently at epoch 67 train accuracy: tensor(0.5929, device='cuda:0') loss of: 1.1477321404457093 eval accuracy: tensor(0.5678, device='cuda:0')\n",
      "currently at epoch 68 train accuracy: tensor(0.5981, device='cuda:0') loss of: 1.137199301815033 eval accuracy: tensor(0.5647, device='cuda:0')\n",
      "currently at epoch 69 train accuracy: tensor(0.5921, device='cuda:0') loss of: 1.1459074883460998 eval accuracy: tensor(0.5634, device='cuda:0')\n",
      "currently at epoch 70 train accuracy: tensor(0.6024, device='cuda:0') loss of: 1.1315329442024231 eval accuracy: tensor(0.5715, device='cuda:0')\n",
      "currently at epoch 71 train accuracy: tensor(0.5993, device='cuda:0') loss of: 1.1297449048995971 eval accuracy: tensor(0.5690, device='cuda:0')\n",
      "currently at epoch 72 train accuracy: tensor(0.5993, device='cuda:0') loss of: 1.1317582078933717 eval accuracy: tensor(0.5667, device='cuda:0')\n",
      "currently at epoch 73 train accuracy: tensor(0.5995, device='cuda:0') loss of: 1.1312843411445617 eval accuracy: tensor(0.5670, device='cuda:0')\n",
      "currently at epoch 74 train accuracy: tensor(0.6024, device='cuda:0') loss of: 1.1215851325035096 eval accuracy: tensor(0.5713, device='cuda:0')\n",
      "currently at epoch 75 train accuracy: tensor(0.6065, device='cuda:0') loss of: 1.1197585438728332 eval accuracy: tensor(0.5717, device='cuda:0')\n",
      "currently at epoch 76 train accuracy: tensor(0.6038, device='cuda:0') loss of: 1.1177774801254272 eval accuracy: tensor(0.5696, device='cuda:0')\n",
      "currently at epoch 77 train accuracy: tensor(0.6037, device='cuda:0') loss of: 1.118376231765747 eval accuracy: tensor(0.5731, device='cuda:0')\n",
      "currently at epoch 78 train accuracy: tensor(0.6052, device='cuda:0') loss of: 1.116987380886078 eval accuracy: tensor(0.5702, device='cuda:0')\n",
      "currently at epoch 79 train accuracy: tensor(0.6083, device='cuda:0') loss of: 1.1103288215637208 eval accuracy: tensor(0.5633, device='cuda:0')\n",
      "currently at epoch 80 train accuracy: tensor(0.6073, device='cuda:0') loss of: 1.109017373275757 eval accuracy: tensor(0.5692, device='cuda:0')\n",
      "currently at epoch 81 train accuracy: tensor(0.6099, device='cuda:0') loss of: 1.1035924224853515 eval accuracy: tensor(0.5829, device='cuda:0')\n",
      "currently at epoch 82 train accuracy: tensor(0.6077, device='cuda:0') loss of: 1.1070722308158873 eval accuracy: tensor(0.5736, device='cuda:0')\n",
      "currently at epoch 83 train accuracy: tensor(0.6096, device='cuda:0') loss of: 1.0972011954307557 eval accuracy: tensor(0.5677, device='cuda:0')\n",
      "currently at epoch 84 train accuracy: tensor(0.6130, device='cuda:0') loss of: 1.094963495540619 eval accuracy: tensor(0.5721, device='cuda:0')\n",
      "currently at epoch 85 train accuracy: tensor(0.6132, device='cuda:0') loss of: 1.0924251017570497 eval accuracy: tensor(0.5755, device='cuda:0')\n",
      "currently at epoch 86 train accuracy: tensor(0.6129, device='cuda:0') loss of: 1.091554183959961 eval accuracy: tensor(0.5780, device='cuda:0')\n",
      "currently at epoch 87 train accuracy: tensor(0.6152, device='cuda:0') loss of: 1.0850496905326843 eval accuracy: tensor(0.5768, device='cuda:0')\n",
      "currently at epoch 88 train accuracy: tensor(0.6176, device='cuda:0') loss of: 1.0800077688217162 eval accuracy: tensor(0.5750, device='cuda:0')\n",
      "currently at epoch 89 train accuracy: tensor(0.6172, device='cuda:0') loss of: 1.0831668308258056 eval accuracy: tensor(0.5798, device='cuda:0')\n",
      "currently at epoch 90 train accuracy: tensor(0.6203, device='cuda:0') loss of: 1.0814841342926025 eval accuracy: tensor(0.5813, device='cuda:0')\n",
      "currently at epoch 91 train accuracy: tensor(0.6160, device='cuda:0') loss of: 1.0819082372665405 eval accuracy: tensor(0.5837, device='cuda:0')\n",
      "currently at epoch 92 train accuracy: tensor(0.6180, device='cuda:0') loss of: 1.0781269934654236 eval accuracy: tensor(0.5743, device='cuda:0')\n",
      "currently at epoch 93 train accuracy: tensor(0.6173, device='cuda:0') loss of: 1.081944467639923 eval accuracy: tensor(0.5748, device='cuda:0')\n",
      "currently at epoch 94 train accuracy: tensor(0.6218, device='cuda:0') loss of: 1.0756722439765931 eval accuracy: tensor(0.5778, device='cuda:0')\n",
      "currently at epoch 95 train accuracy: tensor(0.6235, device='cuda:0') loss of: 1.066612691307068 eval accuracy: tensor(0.5810, device='cuda:0')\n",
      "currently at epoch 96 train accuracy: tensor(0.6258, device='cuda:0') loss of: 1.0593163528442382 eval accuracy: tensor(0.5788, device='cuda:0')\n",
      "currently at epoch 97 train accuracy: tensor(0.6246, device='cuda:0') loss of: 1.0632466272354126 eval accuracy: tensor(0.5750, device='cuda:0')\n",
      "currently at epoch 98 train accuracy: tensor(0.6262, device='cuda:0') loss of: 1.0625506363868713 eval accuracy: tensor(0.5779, device='cuda:0')\n",
      "currently at epoch 99 train accuracy: tensor(0.6256, device='cuda:0') loss of: 1.0596964221954346 eval accuracy: tensor(0.5783, device='cuda:0')\n",
      "currently at epoch 100 train accuracy: tensor(0.6271, device='cuda:0') loss of: 1.0565263837814332 eval accuracy: tensor(0.5728, device='cuda:0')\n",
      "currently at epoch 101 train accuracy: tensor(0.6278, device='cuda:0') loss of: 1.0496574976921083 eval accuracy: tensor(0.5805, device='cuda:0')\n",
      "currently at epoch 102 train accuracy: tensor(0.6272, device='cuda:0') loss of: 1.056308805179596 eval accuracy: tensor(0.5828, device='cuda:0')\n",
      "currently at epoch 103 train accuracy: tensor(0.6264, device='cuda:0') loss of: 1.053254172706604 eval accuracy: tensor(0.5764, device='cuda:0')\n",
      "currently at epoch 104 train accuracy: tensor(0.6318, device='cuda:0') loss of: 1.046998380279541 eval accuracy: tensor(0.5839, device='cuda:0')\n",
      "currently at epoch 105 train accuracy: tensor(0.6297, device='cuda:0') loss of: 1.0436389597892761 eval accuracy: tensor(0.5826, device='cuda:0')\n",
      "currently at epoch 106 train accuracy: tensor(0.6302, device='cuda:0') loss of: 1.0418036051750184 eval accuracy: tensor(0.5823, device='cuda:0')\n",
      "currently at epoch 107 train accuracy: tensor(0.6312, device='cuda:0') loss of: 1.0416667537689208 eval accuracy: tensor(0.5826, device='cuda:0')\n",
      "currently at epoch 108 train accuracy: tensor(0.6335, device='cuda:0') loss of: 1.04273787317276 eval accuracy: tensor(0.5814, device='cuda:0')\n",
      "currently at epoch 109 train accuracy: tensor(0.6338, device='cuda:0') loss of: 1.0368744815826416 eval accuracy: tensor(0.5819, device='cuda:0')\n",
      "currently at epoch 110 train accuracy: tensor(0.6346, device='cuda:0') loss of: 1.0337500535011293 eval accuracy: tensor(0.5871, device='cuda:0')\n",
      "currently at epoch 111 train accuracy: tensor(0.6368, device='cuda:0') loss of: 1.0325752347946167 eval accuracy: tensor(0.5806, device='cuda:0')\n",
      "currently at epoch 112 train accuracy: tensor(0.6353, device='cuda:0') loss of: 1.0304602662086486 eval accuracy: tensor(0.5867, device='cuda:0')\n",
      "currently at epoch 113 train accuracy: tensor(0.6342, device='cuda:0') loss of: 1.0380328243255614 eval accuracy: tensor(0.5868, device='cuda:0')\n",
      "currently at epoch 114 train accuracy: tensor(0.6352, device='cuda:0') loss of: 1.0290002362251283 eval accuracy: tensor(0.5898, device='cuda:0')\n",
      "currently at epoch 115 train accuracy: tensor(0.6397, device='cuda:0') loss of: 1.0193674700736999 eval accuracy: tensor(0.5835, device='cuda:0')\n",
      "currently at epoch 116 train accuracy: tensor(0.6409, device='cuda:0') loss of: 1.018369513988495 eval accuracy: tensor(0.5879, device='cuda:0')\n",
      "currently at epoch 117 train accuracy: tensor(0.6412, device='cuda:0') loss of: 1.0191516386032105 eval accuracy: tensor(0.5822, device='cuda:0')\n",
      "currently at epoch 118 train accuracy: tensor(0.6367, device='cuda:0') loss of: 1.0214490918159485 eval accuracy: tensor(0.5879, device='cuda:0')\n",
      "currently at epoch 119 train accuracy: tensor(0.6411, device='cuda:0') loss of: 1.0175499091148377 eval accuracy: tensor(0.5925, device='cuda:0')\n",
      "currently at epoch 120 train accuracy: tensor(0.6430, device='cuda:0') loss of: 1.0107356602668762 eval accuracy: tensor(0.5882, device='cuda:0')\n",
      "currently at epoch 121 train accuracy: tensor(0.6406, device='cuda:0') loss of: 1.0184389061927794 eval accuracy: tensor(0.5951, device='cuda:0')\n",
      "currently at epoch 122 train accuracy: tensor(0.6456, device='cuda:0') loss of: 1.0104666876792907 eval accuracy: tensor(0.5814, device='cuda:0')\n",
      "currently at epoch 123 train accuracy: tensor(0.6433, device='cuda:0') loss of: 1.0092921792030334 eval accuracy: tensor(0.5828, device='cuda:0')\n",
      "currently at epoch 124 train accuracy: tensor(0.6457, device='cuda:0') loss of: 1.0045316999435425 eval accuracy: tensor(0.5845, device='cuda:0')\n",
      "currently at epoch 125 train accuracy: tensor(0.6455, device='cuda:0') loss of: 1.0045767316818237 eval accuracy: tensor(0.5902, device='cuda:0')\n",
      "currently at epoch 126 train accuracy: tensor(0.6449, device='cuda:0') loss of: 1.0057677756309509 eval accuracy: tensor(0.5943, device='cuda:0')\n",
      "currently at epoch 127 train accuracy: tensor(0.6460, device='cuda:0') loss of: 1.0000006866455078 eval accuracy: tensor(0.5916, device='cuda:0')\n",
      "currently at epoch 128 train accuracy: tensor(0.6453, device='cuda:0') loss of: 1.0020729742050172 eval accuracy: tensor(0.5917, device='cuda:0')\n",
      "currently at epoch 129 train accuracy: tensor(0.6504, device='cuda:0') loss of: 0.993646861743927 eval accuracy: tensor(0.5942, device='cuda:0')\n",
      "currently at epoch 130 train accuracy: tensor(0.6505, device='cuda:0') loss of: 0.994020016002655 eval accuracy: tensor(0.5920, device='cuda:0')\n",
      "currently at epoch 131 train accuracy: tensor(0.6508, device='cuda:0') loss of: 0.9952968580245972 eval accuracy: tensor(0.5894, device='cuda:0')\n",
      "currently at epoch 132 train accuracy: tensor(0.6531, device='cuda:0') loss of: 0.9861026022911071 eval accuracy: tensor(0.5844, device='cuda:0')\n",
      "currently at epoch 133 train accuracy: tensor(0.6508, device='cuda:0') loss of: 0.9877627026557922 eval accuracy: tensor(0.5924, device='cuda:0')\n",
      "currently at epoch 134 train accuracy: tensor(0.6534, device='cuda:0') loss of: 0.9874275581359864 eval accuracy: tensor(0.5888, device='cuda:0')\n",
      "currently at epoch 135 train accuracy: tensor(0.6500, device='cuda:0') loss of: 0.9889781819343567 eval accuracy: tensor(0.5953, device='cuda:0')\n",
      "currently at epoch 136 train accuracy: tensor(0.6531, device='cuda:0') loss of: 0.9809528212547303 eval accuracy: tensor(0.5841, device='cuda:0')\n",
      "currently at epoch 137 train accuracy: tensor(0.6543, device='cuda:0') loss of: 0.9804134950637817 eval accuracy: tensor(0.5925, device='cuda:0')\n",
      "currently at epoch 138 train accuracy: tensor(0.6501, device='cuda:0') loss of: 0.9825455591201783 eval accuracy: tensor(0.5943, device='cuda:0')\n",
      "currently at epoch 139 train accuracy: tensor(0.6532, device='cuda:0') loss of: 0.9841872898101807 eval accuracy: tensor(0.5961, device='cuda:0')\n",
      "currently at epoch 140 train accuracy: tensor(0.6536, device='cuda:0') loss of: 0.9770789644241333 eval accuracy: tensor(0.5899, device='cuda:0')\n",
      "currently at epoch 141 train accuracy: tensor(0.6543, device='cuda:0') loss of: 0.9720836595535278 eval accuracy: tensor(0.5870, device='cuda:0')\n",
      "currently at epoch 142 train accuracy: tensor(0.6543, device='cuda:0') loss of: 0.9798762055397033 eval accuracy: tensor(0.5884, device='cuda:0')\n",
      "currently at epoch 143 train accuracy: tensor(0.6579, device='cuda:0') loss of: 0.9725740489959717 eval accuracy: tensor(0.5928, device='cuda:0')\n",
      "currently at epoch 144 train accuracy: tensor(0.6574, device='cuda:0') loss of: 0.9681567964553833 eval accuracy: tensor(0.5899, device='cuda:0')\n",
      "currently at epoch 145 train accuracy: tensor(0.6578, device='cuda:0') loss of: 0.9682208309173584 eval accuracy: tensor(0.5869, device='cuda:0')\n",
      "currently at epoch 146 train accuracy: tensor(0.6628, device='cuda:0') loss of: 0.960639067363739 eval accuracy: tensor(0.5840, device='cuda:0')\n",
      "currently at epoch 147 train accuracy: tensor(0.6610, device='cuda:0') loss of: 0.9634123851776123 eval accuracy: tensor(0.5915, device='cuda:0')\n",
      "currently at epoch 148 train accuracy: tensor(0.6587, device='cuda:0') loss of: 0.965984106349945 eval accuracy: tensor(0.5941, device='cuda:0')\n",
      "currently at epoch 149 train accuracy: tensor(0.6614, device='cuda:0') loss of: 0.9577388478279114 eval accuracy: tensor(0.5960, device='cuda:0')\n",
      "currently at epoch 150 train accuracy: tensor(0.6622, device='cuda:0') loss of: 0.9606466582298279 eval accuracy: tensor(0.5944, device='cuda:0')\n",
      "currently at epoch 151 train accuracy: tensor(0.6571, device='cuda:0') loss of: 0.9640834457397461 eval accuracy: tensor(0.5912, device='cuda:0')\n",
      "currently at epoch 152 train accuracy: tensor(0.6630, device='cuda:0') loss of: 0.9539979265213012 eval accuracy: tensor(0.5924, device='cuda:0')\n",
      "currently at epoch 153 train accuracy: tensor(0.6611, device='cuda:0') loss of: 0.958523755645752 eval accuracy: tensor(0.6017, device='cuda:0')\n",
      "currently at epoch 154 train accuracy: tensor(0.6632, device='cuda:0') loss of: 0.9538727382659912 eval accuracy: tensor(0.5866, device='cuda:0')\n",
      "currently at epoch 155 train accuracy: tensor(0.6617, device='cuda:0') loss of: 0.9574706517219543 eval accuracy: tensor(0.5939, device='cuda:0')\n",
      "currently at epoch 156 train accuracy: tensor(0.6669, device='cuda:0') loss of: 0.9458248902320862 eval accuracy: tensor(0.6008, device='cuda:0')\n",
      "currently at epoch 157 train accuracy: tensor(0.6657, device='cuda:0') loss of: 0.9445284465789795 eval accuracy: tensor(0.5948, device='cuda:0')\n",
      "currently at epoch 158 train accuracy: tensor(0.6665, device='cuda:0') loss of: 0.9413310890197754 eval accuracy: tensor(0.5984, device='cuda:0')\n",
      "currently at epoch 159 train accuracy: tensor(0.6680, device='cuda:0') loss of: 0.9379600010871887 eval accuracy: tensor(0.5915, device='cuda:0')\n",
      "currently at epoch 160 train accuracy: tensor(0.6683, device='cuda:0') loss of: 0.9434249675750732 eval accuracy: tensor(0.5978, device='cuda:0')\n",
      "currently at epoch 161 train accuracy: tensor(0.6658, device='cuda:0') loss of: 0.9471682873725891 eval accuracy: tensor(0.6010, device='cuda:0')\n",
      "currently at epoch 162 train accuracy: tensor(0.6673, device='cuda:0') loss of: 0.9378980973243713 eval accuracy: tensor(0.5980, device='cuda:0')\n",
      "currently at epoch 163 train accuracy: tensor(0.6694, device='cuda:0') loss of: 0.9354124801635743 eval accuracy: tensor(0.5935, device='cuda:0')\n",
      "currently at epoch 164 train accuracy: tensor(0.6711, device='cuda:0') loss of: 0.9282412504196167 eval accuracy: tensor(0.5951, device='cuda:0')\n",
      "currently at epoch 165 train accuracy: tensor(0.6716, device='cuda:0') loss of: 0.9336565588951111 eval accuracy: tensor(0.6004, device='cuda:0')\n",
      "currently at epoch 166 train accuracy: tensor(0.6698, device='cuda:0') loss of: 0.9294272143363953 eval accuracy: tensor(0.5974, device='cuda:0')\n",
      "currently at epoch 167 train accuracy: tensor(0.6695, device='cuda:0') loss of: 0.9314398292541504 eval accuracy: tensor(0.6021, device='cuda:0')\n",
      "currently at epoch 168 train accuracy: tensor(0.6712, device='cuda:0') loss of: 0.9280026822090149 eval accuracy: tensor(0.5984, device='cuda:0')\n",
      "currently at epoch 169 train accuracy: tensor(0.6706, device='cuda:0') loss of: 0.9318251805305481 eval accuracy: tensor(0.5992, device='cuda:0')\n",
      "currently at epoch 170 train accuracy: tensor(0.6761, device='cuda:0') loss of: 0.9210584343910218 eval accuracy: tensor(0.5920, device='cuda:0')\n",
      "currently at epoch 171 train accuracy: tensor(0.6704, device='cuda:0') loss of: 0.9325273456573486 eval accuracy: tensor(0.5889, device='cuda:0')\n",
      "currently at epoch 172 train accuracy: tensor(0.6700, device='cuda:0') loss of: 0.9309805577278137 eval accuracy: tensor(0.5983, device='cuda:0')\n",
      "currently at epoch 173 train accuracy: tensor(0.6747, device='cuda:0') loss of: 0.9210067007064819 eval accuracy: tensor(0.5963, device='cuda:0')\n",
      "currently at epoch 174 train accuracy: tensor(0.6761, device='cuda:0') loss of: 0.9186103521347045 eval accuracy: tensor(0.6037, device='cuda:0')\n",
      "currently at epoch 175 train accuracy: tensor(0.6718, device='cuda:0') loss of: 0.9215065861701965 eval accuracy: tensor(0.5951, device='cuda:0')\n",
      "currently at epoch 176 train accuracy: tensor(0.6729, device='cuda:0') loss of: 0.9230832813262939 eval accuracy: tensor(0.6031, device='cuda:0')\n",
      "currently at epoch 177 train accuracy: tensor(0.6739, device='cuda:0') loss of: 0.9210106798171998 eval accuracy: tensor(0.6066, device='cuda:0')\n",
      "currently at epoch 178 train accuracy: tensor(0.6798, device='cuda:0') loss of: 0.9086495198249817 eval accuracy: tensor(0.6015, device='cuda:0')\n",
      "currently at epoch 179 train accuracy: tensor(0.6765, device='cuda:0') loss of: 0.9125003847122193 eval accuracy: tensor(0.5953, device='cuda:0')\n",
      "currently at epoch 180 train accuracy: tensor(0.6758, device='cuda:0') loss of: 0.9174383535385132 eval accuracy: tensor(0.5945, device='cuda:0')\n",
      "currently at epoch 181 train accuracy: tensor(0.6782, device='cuda:0') loss of: 0.9083859828948975 eval accuracy: tensor(0.5976, device='cuda:0')\n",
      "currently at epoch 182 train accuracy: tensor(0.6769, device='cuda:0') loss of: 0.9135386298179626 eval accuracy: tensor(0.5993, device='cuda:0')\n",
      "currently at epoch 183 train accuracy: tensor(0.6772, device='cuda:0') loss of: 0.9098511004447937 eval accuracy: tensor(0.6065, device='cuda:0')\n",
      "currently at epoch 184 train accuracy: tensor(0.6790, device='cuda:0') loss of: 0.9092942701339721 eval accuracy: tensor(0.5950, device='cuda:0')\n",
      "currently at epoch 185 train accuracy: tensor(0.6815, device='cuda:0') loss of: 0.8956151650428772 eval accuracy: tensor(0.6029, device='cuda:0')\n",
      "currently at epoch 186 train accuracy: tensor(0.6792, device='cuda:0') loss of: 0.9043921760559082 eval accuracy: tensor(0.5971, device='cuda:0')\n",
      "currently at epoch 187 train accuracy: tensor(0.6799, device='cuda:0') loss of: 0.8997661314964295 eval accuracy: tensor(0.6020, device='cuda:0')\n",
      "currently at epoch 188 train accuracy: tensor(0.6790, device='cuda:0') loss of: 0.9003993014335633 eval accuracy: tensor(0.5949, device='cuda:0')\n",
      "currently at epoch 189 train accuracy: tensor(0.6866, device='cuda:0') loss of: 0.8931661748886108 eval accuracy: tensor(0.6006, device='cuda:0')\n",
      "currently at epoch 190 train accuracy: tensor(0.6806, device='cuda:0') loss of: 0.9028465689659119 eval accuracy: tensor(0.5994, device='cuda:0')\n",
      "currently at epoch 191 train accuracy: tensor(0.6809, device='cuda:0') loss of: 0.8949327903747558 eval accuracy: tensor(0.6007, device='cuda:0')\n",
      "currently at epoch 192 train accuracy: tensor(0.6840, device='cuda:0') loss of: 0.8997266761779785 eval accuracy: tensor(0.5976, device='cuda:0')\n",
      "currently at epoch 193 train accuracy: tensor(0.6828, device='cuda:0') loss of: 0.8983983008384705 eval accuracy: tensor(0.5961, device='cuda:0')\n",
      "currently at epoch 194 train accuracy: tensor(0.6863, device='cuda:0') loss of: 0.8869584607124329 eval accuracy: tensor(0.5998, device='cuda:0')\n",
      "currently at epoch 195 train accuracy: tensor(0.6846, device='cuda:0') loss of: 0.8910773636817932 eval accuracy: tensor(0.6068, device='cuda:0')\n",
      "currently at epoch 196 train accuracy: tensor(0.6861, device='cuda:0') loss of: 0.8896530136108398 eval accuracy: tensor(0.5983, device='cuda:0')\n",
      "currently at epoch 197 train accuracy: tensor(0.6862, device='cuda:0') loss of: 0.8891095446586609 eval accuracy: tensor(0.6049, device='cuda:0')\n",
      "currently at epoch 198 train accuracy: tensor(0.6833, device='cuda:0') loss of: 0.8898865797996521 eval accuracy: tensor(0.6024, device='cuda:0')\n",
      "currently at epoch 199 train accuracy: tensor(0.6852, device='cuda:0') loss of: 0.8889759559631347 eval accuracy: tensor(0.5998, device='cuda:0')\n",
      "currently at epoch 200 train accuracy: tensor(0.6887, device='cuda:0') loss of: 0.8855387383460999 eval accuracy: tensor(0.6012, device='cuda:0')\n",
      "currently at epoch 201 train accuracy: tensor(0.6886, device='cuda:0') loss of: 0.8806889904975891 eval accuracy: tensor(0.5991, device='cuda:0')\n",
      "currently at epoch 202 train accuracy: tensor(0.6892, device='cuda:0') loss of: 0.8776831336975097 eval accuracy: tensor(0.6045, device='cuda:0')\n",
      "currently at epoch 203 train accuracy: tensor(0.6895, device='cuda:0') loss of: 0.878000765800476 eval accuracy: tensor(0.5970, device='cuda:0')\n",
      "currently at epoch 204 train accuracy: tensor(0.6892, device='cuda:0') loss of: 0.8774111932754517 eval accuracy: tensor(0.6004, device='cuda:0')\n",
      "currently at epoch 205 train accuracy: tensor(0.6892, device='cuda:0') loss of: 0.8773817802429199 eval accuracy: tensor(0.6005, device='cuda:0')\n",
      "currently at epoch 206 train accuracy: tensor(0.6931, device='cuda:0') loss of: 0.8679978440284729 eval accuracy: tensor(0.6029, device='cuda:0')\n",
      "currently at epoch 207 train accuracy: tensor(0.6896, device='cuda:0') loss of: 0.8701827528953552 eval accuracy: tensor(0.5932, device='cuda:0')\n",
      "currently at epoch 208 train accuracy: tensor(0.6887, device='cuda:0') loss of: 0.8730924640655517 eval accuracy: tensor(0.6030, device='cuda:0')\n",
      "currently at epoch 209 train accuracy: tensor(0.6914, device='cuda:0') loss of: 0.8732904635429383 eval accuracy: tensor(0.5975, device='cuda:0')\n",
      "currently at epoch 210 train accuracy: tensor(0.6913, device='cuda:0') loss of: 0.8698166293621064 eval accuracy: tensor(0.6033, device='cuda:0')\n",
      "currently at epoch 211 train accuracy: tensor(0.6896, device='cuda:0') loss of: 0.874904048538208 eval accuracy: tensor(0.5990, device='cuda:0')\n",
      "currently at epoch 212 train accuracy: tensor(0.6905, device='cuda:0') loss of: 0.871503965997696 eval accuracy: tensor(0.6001, device='cuda:0')\n",
      "currently at epoch 213 train accuracy: tensor(0.6902, device='cuda:0') loss of: 0.8725185984611511 eval accuracy: tensor(0.6046, device='cuda:0')\n",
      "currently at epoch 214 train accuracy: tensor(0.6937, device='cuda:0') loss of: 0.8620714584350586 eval accuracy: tensor(0.6088, device='cuda:0')\n",
      "currently at epoch 215 train accuracy: tensor(0.6933, device='cuda:0') loss of: 0.8642160225868225 eval accuracy: tensor(0.5983, device='cuda:0')\n",
      "currently at epoch 216 train accuracy: tensor(0.6967, device='cuda:0') loss of: 0.8603283288002014 eval accuracy: tensor(0.6013, device='cuda:0')\n",
      "currently at epoch 217 train accuracy: tensor(0.6952, device='cuda:0') loss of: 0.8647575508117675 eval accuracy: tensor(0.6064, device='cuda:0')\n",
      "currently at epoch 218 train accuracy: tensor(0.6950, device='cuda:0') loss of: 0.8649616709709167 eval accuracy: tensor(0.5947, device='cuda:0')\n",
      "currently at epoch 219 train accuracy: tensor(0.6968, device='cuda:0') loss of: 0.8524370333194733 eval accuracy: tensor(0.6001, device='cuda:0')\n",
      "currently at epoch 220 train accuracy: tensor(0.6962, device='cuda:0') loss of: 0.855976445198059 eval accuracy: tensor(0.6089, device='cuda:0')\n",
      "currently at epoch 221 train accuracy: tensor(0.6987, device='cuda:0') loss of: 0.8505927141189575 eval accuracy: tensor(0.6065, device='cuda:0')\n",
      "currently at epoch 222 train accuracy: tensor(0.6959, device='cuda:0') loss of: 0.8569498578071594 eval accuracy: tensor(0.6067, device='cuda:0')\n",
      "currently at epoch 223 train accuracy: tensor(0.6942, device='cuda:0') loss of: 0.8607189941883087 eval accuracy: tensor(0.6005, device='cuda:0')\n",
      "currently at epoch 224 train accuracy: tensor(0.6980, device='cuda:0') loss of: 0.8536757863998413 eval accuracy: tensor(0.6035, device='cuda:0')\n",
      "currently at epoch 225 train accuracy: tensor(0.7011, device='cuda:0') loss of: 0.8415467409610748 eval accuracy: tensor(0.6045, device='cuda:0')\n",
      "currently at epoch 226 train accuracy: tensor(0.7000, device='cuda:0') loss of: 0.846762332201004 eval accuracy: tensor(0.6051, device='cuda:0')\n",
      "currently at epoch 227 train accuracy: tensor(0.6999, device='cuda:0') loss of: 0.8471067439079285 eval accuracy: tensor(0.6097, device='cuda:0')\n",
      "currently at epoch 228 train accuracy: tensor(0.7009, device='cuda:0') loss of: 0.8486257013320923 eval accuracy: tensor(0.6038, device='cuda:0')\n",
      "currently at epoch 229 train accuracy: tensor(0.7021, device='cuda:0') loss of: 0.8455843690872192 eval accuracy: tensor(0.6059, device='cuda:0')\n",
      "currently at epoch 230 train accuracy: tensor(0.6989, device='cuda:0') loss of: 0.8495762930870057 eval accuracy: tensor(0.6063, device='cuda:0')\n",
      "currently at epoch 231 train accuracy: tensor(0.7029, device='cuda:0') loss of: 0.8376871392250061 eval accuracy: tensor(0.6037, device='cuda:0')\n",
      "currently at epoch 232 train accuracy: tensor(0.7021, device='cuda:0') loss of: 0.8431746279716492 eval accuracy: tensor(0.6007, device='cuda:0')\n",
      "currently at epoch 233 train accuracy: tensor(0.7046, device='cuda:0') loss of: 0.8383816147327423 eval accuracy: tensor(0.6066, device='cuda:0')\n",
      "currently at epoch 234 train accuracy: tensor(0.7029, device='cuda:0') loss of: 0.8365861402988434 eval accuracy: tensor(0.6042, device='cuda:0')\n",
      "currently at epoch 235 train accuracy: tensor(0.7022, device='cuda:0') loss of: 0.8433223880290985 eval accuracy: tensor(0.6014, device='cuda:0')\n",
      "currently at epoch 236 train accuracy: tensor(0.6999, device='cuda:0') loss of: 0.8405195534229278 eval accuracy: tensor(0.6008, device='cuda:0')\n",
      "currently at epoch 237 train accuracy: tensor(0.7034, device='cuda:0') loss of: 0.8388025288581848 eval accuracy: tensor(0.6089, device='cuda:0')\n",
      "currently at epoch 238 train accuracy: tensor(0.7044, device='cuda:0') loss of: 0.8361758540153503 eval accuracy: tensor(0.6024, device='cuda:0')\n",
      "currently at epoch 239 train accuracy: tensor(0.7064, device='cuda:0') loss of: 0.8307569021224975 eval accuracy: tensor(0.6051, device='cuda:0')\n",
      "currently at epoch 240 train accuracy: tensor(0.7076, device='cuda:0') loss of: 0.8280668162345887 eval accuracy: tensor(0.6062, device='cuda:0')\n",
      "currently at epoch 241 train accuracy: tensor(0.7040, device='cuda:0') loss of: 0.833104518032074 eval accuracy: tensor(0.5983, device='cuda:0')\n",
      "currently at epoch 242 train accuracy: tensor(0.7058, device='cuda:0') loss of: 0.8286026713371277 eval accuracy: tensor(0.6014, device='cuda:0')\n",
      "currently at epoch 243 train accuracy: tensor(0.7054, device='cuda:0') loss of: 0.8294823764801026 eval accuracy: tensor(0.6054, device='cuda:0')\n",
      "currently at epoch 244 train accuracy: tensor(0.7030, device='cuda:0') loss of: 0.8297519199848175 eval accuracy: tensor(0.5990, device='cuda:0')\n",
      "currently at epoch 245 train accuracy: tensor(0.7088, device='cuda:0') loss of: 0.8203996453762055 eval accuracy: tensor(0.6047, device='cuda:0')\n",
      "currently at epoch 246 train accuracy: tensor(0.7071, device='cuda:0') loss of: 0.8270306494235993 eval accuracy: tensor(0.6055, device='cuda:0')\n",
      "currently at epoch 247 train accuracy: tensor(0.7066, device='cuda:0') loss of: 0.8233772599697113 eval accuracy: tensor(0.6029, device='cuda:0')\n",
      "currently at epoch 248 train accuracy: tensor(0.7105, device='cuda:0') loss of: 0.8232044990062714 eval accuracy: tensor(0.6015, device='cuda:0')\n",
      "currently at epoch 249 train accuracy: tensor(0.7119, device='cuda:0') loss of: 0.8203260826587677 eval accuracy: tensor(0.6067, device='cuda:0')\n",
      "testing accuracy of InceptionModel2_data_aug_cropping is: tensor(0.6013, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = InceptionModel3(channels_in=3, class_num=10)\n",
    "modelName = 'InceptionModel3_crop_flip'\n",
    "\n",
    "# this transformation sequence must be modified to work with the random stuff\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL Image to PyTorch Tensor\n",
    "    # this normalization is industry standard, it seems\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomCrop(size=28, padding=2, pad_if_needed=True, padding_mode=\"edge\"),\n",
    "    transforms.Resize(32),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # Normalize the data\n",
    "])\n",
    "\n",
    "argDict = {\n",
    "    'lr': 0.001,\n",
    "    'maxEpoch': 500,\n",
    "    'idleEpoch': 25,\n",
    "    'outputName': modelName,\n",
    "    'optimizer': optim.SGD(model.parameters(), lr=0.001),\n",
    "    'criterion': nn.CrossEntropyLoss()\n",
    "}\n",
    "\n",
    "STUDENTID = 567     # this will be used for random states\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import logging\n",
    "from utils import *\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "# define the label transformations, from int64 to float32\n",
    "transform_label = transforms.Compose([\n",
    "    #transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Download and load CIFAR-100 datasets\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True, target_transform=transform_label)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform, download=True, target_transform=transform_label)\n",
    "\n",
    "\n",
    "# Calculate the sizes for train, validation, and test sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "valid_size = len(train_dataset) - train_size\n",
    "\n",
    "# Split the train dataset into train and validation sets using a random seed\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [train_size, valid_size], generator=torch.Generator().manual_seed(STUDENTID))\n",
    "\n",
    "# Create data loaders for training, validation, and test sets\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(valid_dataset))\n",
    "print(\"Number of test examples:\", len(test_dataset))\n",
    "\n",
    "# creating a model that automatically runs the forward function i guess, since it is easier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils import *\n",
    "from customModels import *\n",
    "\n",
    "# setting up the logger\n",
    "loggerName = modelName + '.log'\n",
    "loggerName = os.path.join(argDict['outputName'], loggerName)\n",
    "logger = MyLogger(loggerName)\n",
    "argDict['logger'] = logger\n",
    "\n",
    "# just to initilalize the files\n",
    "logger.log('training starts here')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# training and saving model to dictionary\n",
    "outputDict = train(model, argDict, train_loader, val_loader, test_loader)\n",
    "\n",
    "# loading the best model, and then sending it off to testing\n",
    "model = load_model_from_file(model, argDict['outputName'], argDict['outputName'])\n",
    "\n",
    "test_accuracy = test(model, argDict, test_loader)\n",
    "tempString = 'testing accuracy of ' + argDict['outputName'] + \" is: \" + str(test_accuracy)\n",
    "logger.log(tempString)\n",
    "\n",
    "argDict['test_accuracy'] = str(test_accuracy)\n",
    "\n",
    "# timing the thing as well\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "argDict['time_taken'] = execution_time\n",
    "save_dict_to_file(outputDict, argDict['outputName'], argDict['outputName'])\n",
    "\n",
    "del model\n",
    "del argDict\n",
    "\n",
    "# Define the folder you want to zip and the output zip file name\n",
    "folder_to_zip = modelName\n",
    "output_zip_file = modelName + \".zip\"\n",
    "\n",
    "# Use shutil.make_archive to create the zip file\n",
    "shutil.make_archive(output_zip_file, 'zip', folder_to_zip)\n",
    "\n",
    "os.rename(output_zip_file + '.zip', output_zip_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}