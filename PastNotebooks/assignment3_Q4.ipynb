{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    " # Question 4, taking the template of question 3 i guess"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import shapiro\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Input, Dense, SimpleRNN, GRU, LSTM, Dropout, Activation, BatchNormalization, LayerNormalization, Dropout\n",
    "from keras import initializers, regularizers, optimizers, backend\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import shapiro\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Dropout, Activation, BatchNormalization, Dropout\n",
    "from keras import initializers, regularizers, optimizers, backend\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ANN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41757, 5) (41757, 1)\n",
      "(41781, 5) (41781, 1)\n",
      "(41733, 5) (41733, 1)\n",
      "timestep of 50\n",
      "Mean squared error of the scaled testing dataset: 0.01313077854279026\n",
      "Mean squared error of the real testing dataset (USD): 12973.681923665597\n",
      "(41757, 5) (41757, 1)\n",
      "(41829, 5) (41829, 1)\n",
      "(41685, 5) (41685, 1)\n",
      "timestep of 50\n",
      "Mean squared error of the scaled testing dataset: 0.02009933836980414\n",
      "Mean squared error of the real testing dataset (USD): 19858.86988439936\n",
      "(41757, 5) (41757, 1)\n",
      "(41925, 5) (41925, 1)\n",
      "(41589, 5) (41589, 1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [3], line 87\u001B[0m\n\u001B[0;32m     81\u001B[0m x_test, y_test \u001B[38;5;241m=\u001B[39m data_generator[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     86\u001B[0m \u001B[38;5;66;03m# network section starts here\u001B[39;00m\n\u001B[1;32m---> 87\u001B[0m \u001B[43mbackend\u001B[49m\u001B[38;5;241m.\u001B[39mclear_session()\n\u001B[0;32m     88\u001B[0m init_weight \u001B[38;5;241m=\u001B[39m initializers\u001B[38;5;241m.\u001B[39mGlorotUniform(seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m7567\u001B[39m)\n\u001B[0;32m     89\u001B[0m regu_weight \u001B[38;5;241m=\u001B[39m regularizers\u001B[38;5;241m.\u001B[39ml2(\u001B[38;5;241m0.01\u001B[39m)\n",
      "Cell \u001B[1;32mIn [3], line 87\u001B[0m\n\u001B[0;32m     81\u001B[0m x_test, y_test \u001B[38;5;241m=\u001B[39m data_generator[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     86\u001B[0m \u001B[38;5;66;03m# network section starts here\u001B[39;00m\n\u001B[1;32m---> 87\u001B[0m \u001B[43mbackend\u001B[49m\u001B[38;5;241m.\u001B[39mclear_session()\n\u001B[0;32m     88\u001B[0m init_weight \u001B[38;5;241m=\u001B[39m initializers\u001B[38;5;241m.\u001B[39mGlorotUniform(seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m7567\u001B[39m)\n\u001B[0;32m     89\u001B[0m regu_weight \u001B[38;5;241m=\u001B[39m regularizers\u001B[38;5;241m.\u001B[39ml2(\u001B[38;5;241m0.01\u001B[39m)\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:1180\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:621\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:930\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:921\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:318\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\DataSpell 2021.3.3\\plugins\\python-ce\\helpers\\pydev\\pydevd.py:1147\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1144\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1146\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1147\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\DataSpell 2021.3.3\\plugins\\python-ce\\helpers\\pydev\\pydevd.py:1162\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1159\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1161\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1162\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1164\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1166\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# cleaning out the NaNs\n",
    "# this will be loading in the regression dataset\n",
    "# Replace 'your_file.csv' with the path to your CSV file\n",
    "file_path = 'data/Air_pollution.csv'\n",
    "\n",
    "# Use pandas to read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "time_offset_list = [24, 72, 168]\n",
    "mse_train_list = []\n",
    "mse_val_list = []\n",
    "mse_test_list = []\n",
    "\n",
    "for time_offset in time_offset_list:\n",
    "    # taking out the nessesary data\n",
    "    new_df = df[['month', 'day', 'hour', 'pm2.5', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir']]\n",
    "\n",
    "    # dropping out the NaN numbers i guess\n",
    "    new_df = new_df.dropna()\n",
    "\n",
    "    # data preprocessing\n",
    "    # using default values + month, since that was what worked best i guess\n",
    "    asset_input = pd.DataFrame(new_df, columns=['month', 'DEWP', 'TEMP', 'PRES', 'Iws'])\n",
    "    x_all = asset_input.values\n",
    "\n",
    "    asset_output = pd.DataFrame(new_df, columns=['pm2.5'])\n",
    "    y_all = asset_output.values\n",
    "\n",
    "    # finding out the length of the dataset, and then getting the first 90% as the data i guess\n",
    "    splits = int(0.9 * x_all.shape[0])\n",
    "\n",
    "    timestep = 50\n",
    "\n",
    "    time_offset = time_offset\n",
    "\n",
    "    # in this case, i have to offset the training data by 24 hours, 72, and 168 hours. I will be doing this by adding empty zero rows into the data, and then trimming data off the end so that no invalid data is obtained i guess\n",
    "    # bumping the x data forward by 24 instead of pushing y back by 24 i guess\n",
    "    print(x_all.shape, y_all.shape)\n",
    "    zeros_to_append = np.zeros((time_offset, 5))\n",
    "    x_all = np.concatenate((zeros_to_append, x_all))\n",
    "    zeros_to_append = np.zeros((time_offset, 1))\n",
    "    y_all = np.concatenate((y_all, zeros_to_append))\n",
    "    print(x_all.shape, y_all.shape)\n",
    "\n",
    "    # slicing out the useless zero data at the end and at the start. The array should now be smaller, but should be offsetted the right amount\n",
    "    # idk how the nan values are gonna affect this. Hopefully it does not affect it too much\n",
    "    x_all = x_all[time_offset:-time_offset, :]\n",
    "    y_all = y_all[time_offset:-time_offset, :]\n",
    "    print(x_all.shape, y_all.shape)\n",
    "\n",
    "    x_real = x_all[0:splits, :]\n",
    "    y_real = y_all[0:splits, :]\n",
    "\n",
    "    # testing samples\n",
    "    x_test_real = x_all[splits-timestep:, :]\n",
    "    y_test_real = y_all[splits-timestep:, :]\n",
    "\n",
    "    # scaling the samples accordingly\n",
    "    # most of them fall within a range, and has min and max value, and as such, normalization is choosen i guess\n",
    "    # Normalization\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_x.fit(x_real)\n",
    "    x_scale = scaler_x.transform(x_real)\n",
    "\n",
    "    scaler_y = MinMaxScaler()\n",
    "    scaler_y.fit(y_real)\n",
    "    y_scale = scaler_y.transform(y_real)\n",
    "\n",
    "    # setting up the training and validation dataset\n",
    "    data_generator = TimeseriesGenerator(x_scale, y_scale, length=timestep, sampling_rate=1, batch_size=10000)\n",
    "    x_timestep, y_timestep = data_generator[0]\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_timestep, y_timestep, test_size=0.1, random_state=7567)\n",
    "\n",
    "\n",
    "    # generating the testing data as well i guess\n",
    "    x_test_scale = scaler_x.transform(x_test_real)\n",
    "    y_test_scale = scaler_y.transform(y_test_real)\n",
    "\n",
    "    data_generator = TimeseriesGenerator(x_test_scale, y_test_scale, length=timestep, sampling_rate=1, batch_size=10000)\n",
    "    x_test, y_test = data_generator[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # network section starts here\n",
    "    backend.clear_session()\n",
    "    init_weight = initializers.GlorotUniform(seed=7567)\n",
    "    regu_weight = regularizers.l2(0.01)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(timestep,5)))\n",
    "    model.add(Dense(100, kernel_initializer=init_weight, kernel_regularizer=regu_weight))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('ReLU'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, kernel_initializer=init_weight, kernel_regularizer=regu_weight))\n",
    "\n",
    "    my_optmz = optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=my_optmz, loss='mean_squared_error', metrics='mse')\n",
    "    model_hist = model.fit(x_train, y_train, epochs=20, batch_size=64, validation_data=(x_test,y_test), verbose=0)\n",
    "\n",
    "    # predict the train and validation as well i guess\n",
    "\n",
    "\n",
    "    y_test_pred_scale = model.predict(x_test, verbose=0)\n",
    "    mse_test_scale = mean_squared_error(y_test, y_test_pred_scale)\n",
    "\n",
    "    y_test_pred_real = scaler_y.inverse_transform(y_test_pred_scale)\n",
    "    mse_test_real = mean_squared_error(y_test_real[timestep:,0], y_test_pred_real)\n",
    "\n",
    "\n",
    "    mse_train_list.append(model_hist.history['mse'][-1])\n",
    "    mse_val_list.append(model_hist.history['val_mse'][-1])\n",
    "    # grabbing the scaled mse\n",
    "    mse_test_list.append(mse_test_scale)\n",
    "\n",
    "    print(f'timestep of {timestep}')\n",
    "    print(\"Mean squared error of the scaled testing dataset:\", mse_test_scale)\n",
    "    print(\"Mean squared error of the real testing dataset (USD):\", mse_test_real)\n",
    "\n",
    "print(mse_train_list)\n",
    "print(mse_val_list)\n",
    "print(mse_test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    " # RNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 50)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               5100      \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 100)               400       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " activation (Activation)     (None, 100)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5601 (21.88 KB)\n",
      "Trainable params: 5401 (21.10 KB)\n",
      "Non-trainable params: 200 (800.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# cleaning out the NaNs\n",
    "# this will be loading in the regression dataset\n",
    "# Replace 'your_file.csv' with the path to your CSV file\n",
    "file_path = 'data/Air_pollution.csv'\n",
    "\n",
    "# Use pandas to read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "time_offset_list = [24, 72, 168]\n",
    "mse_train_list = []\n",
    "mse_val_list = []\n",
    "mse_test_list = []\n",
    "\n",
    "for time_offset in time_offset_list:\n",
    "    # taking out the nessesary data\n",
    "    new_df = df[['month', 'day', 'hour', 'pm2.5', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir']]\n",
    "\n",
    "    # dropping out the NaN numbers i guess\n",
    "    new_df = new_df.dropna()\n",
    "\n",
    "    # data preprocessing\n",
    "    # using default values + month, since that was what worked best i guess\n",
    "    asset_input = pd.DataFrame(new_df, columns=['month', 'DEWP', 'TEMP', 'PRES', 'Iws'])\n",
    "    x_all = asset_input.values\n",
    "\n",
    "    asset_output = pd.DataFrame(new_df, columns=['pm2.5'])\n",
    "    y_all = asset_output.values\n",
    "\n",
    "    # finding out the length of the dataset, and then getting the first 90% as the data i guess\n",
    "    splits = int(0.9 * x_all.shape[0])\n",
    "\n",
    "    timestep = 50\n",
    "\n",
    "    time_offset = time_offset\n",
    "\n",
    "    # in this case, i have to offset the training data by 24 hours, 72, and 168 hours. I will be doing this by adding empty zero rows into the data, and then trimming data off the end so that no invalid data is obtained i guess\n",
    "    # bumping the x data forward by 24 instead of pushing y back by 24 i guess\n",
    "    print(x_all.shape, y_all.shape)\n",
    "    zeros_to_append = np.zeros((time_offset, 5))\n",
    "    x_all = np.concatenate((zeros_to_append, x_all))\n",
    "    zeros_to_append = np.zeros((time_offset, 1))\n",
    "    y_all = np.concatenate((y_all, zeros_to_append))\n",
    "    print(x_all.shape, y_all.shape)\n",
    "\n",
    "    # slicing out the useless zero data at the end and at the start. The array should now be smaller, but should be offsetted the right amount\n",
    "    # idk how the nan values are gonna affect this. Hopefully it does not affect it too much\n",
    "    x_all = x_all[time_offset:-time_offset, :]\n",
    "    y_all = y_all[time_offset:-time_offset, :]\n",
    "    print(x_all.shape, y_all.shape)\n",
    "\n",
    "    x_real = x_all[0:splits, :]\n",
    "    y_real = y_all[0:splits, :]\n",
    "\n",
    "    # testing samples\n",
    "    x_test_real = x_all[splits-timestep:, :]\n",
    "    y_test_real = y_all[splits-timestep:, :]\n",
    "\n",
    "    # scaling the samples accordingly\n",
    "    # most of them fall within a range, and has min and max value, and as such, normalization is choosen i guess\n",
    "    # Normalization\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_x.fit(x_real)\n",
    "    x_scale = scaler_x.transform(x_real)\n",
    "\n",
    "    scaler_y = MinMaxScaler()\n",
    "    scaler_y.fit(y_real)\n",
    "    y_scale = scaler_y.transform(y_real)\n",
    "\n",
    "    # setting up the training and validation dataset\n",
    "    data_generator = TimeseriesGenerator(x_scale, y_scale, length=timestep, sampling_rate=1, batch_size=10000)\n",
    "    x_timestep, y_timestep = data_generator[0]\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_timestep, y_timestep, test_size=0.1, random_state=7567)\n",
    "\n",
    "\n",
    "    # generating the testing data as well i guess\n",
    "    x_test_scale = scaler_x.transform(x_test_real)\n",
    "    y_test_scale = scaler_y.transform(y_test_real)\n",
    "\n",
    "    data_generator = TimeseriesGenerator(x_test_scale, y_test_scale, length=timestep, sampling_rate=1, batch_size=10000)\n",
    "    x_test, y_test = data_generator[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # network section starts here\n",
    "    backend.clear_session()\n",
    "    init_weight = initializers.GlorotUniform(seed=7567)\n",
    "    regu_weight = regularizers.l2(0.001)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(timestep,5)))\n",
    "    model.add(SimpleRNN(100, activation='tanh', return_sequences=False, kernel_initializer=init_weight, kernel_regularizer=regu_weight, recurrent_dropout=0))\n",
    "    #model.add(LayerNormalization()) #optional, default activation must be changed to None\n",
    "    #model.add(Activation('tanh')) #separately apply activation function if using layer normalization\n",
    "    model.add(Dense(1, kernel_initializer=init_weight, kernel_regularizer=regu_weight))\n",
    "\n",
    "    my_optmz = optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=my_optmz, loss='mean_squared_error', metrics='mse')\n",
    "    model_hist = model.fit(x_train, y_train, epochs=30, batch_size=64, validation_data=(x_val,y_val), verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # predict the train and validation as well i guess\n",
    "    y_test_pred_scale = model.predict(x_test, verbose=0)\n",
    "    mse_test_scale = mean_squared_error(y_test, y_test_pred_scale)\n",
    "\n",
    "    y_test_pred_real = scaler_y.inverse_transform(y_test_pred_scale)\n",
    "    mse_test_real = mean_squared_error(y_test_real[timestep:,0], y_test_pred_real)\n",
    "\n",
    "\n",
    "    mse_train_list.append(model_hist.history['mse'][-1])\n",
    "    mse_val_list.append(model_hist.history['val_mse'][-1])\n",
    "    # grabbing the scaled mse\n",
    "    mse_test_list.append(mse_test_scale)\n",
    "\n",
    "    print(f'timestep of {timestep}')\n",
    "    print(\"Mean squared error of the scaled testing dataset:\", mse_test_scale)\n",
    "    print(\"Mean squared error of the real testing dataset (USD):\", mse_test_real)\n",
    "\n",
    "print(mse_train_list)\n",
    "print(mse_val_list)\n",
    "print(mse_test_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gated recurrent neural network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# cleaning out the NaNs\n",
    "# this will be loading in the regression dataset\n",
    "# Replace 'your_file.csv' with the path to your CSV file\n",
    "file_path = 'data/Air_pollution.csv'\n",
    "\n",
    "# Use pandas to read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "time_offset_list = [24, 72, 168]\n",
    "mse_train_list = []\n",
    "mse_val_list = []\n",
    "mse_test_list = []\n",
    "\n",
    "for time_offset in time_offset_list:\n",
    "    # taking out the nessesary data\n",
    "    new_df = df[['month', 'day', 'hour', 'pm2.5', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir']]\n",
    "\n",
    "    # dropping out the NaN numbers i guess\n",
    "    new_df = new_df.dropna()\n",
    "\n",
    "    # data preprocessing\n",
    "    # using default values + month, since that was what worked best i guess\n",
    "    asset_input = pd.DataFrame(new_df, columns=['month', 'DEWP', 'TEMP', 'PRES', 'Iws'])\n",
    "    x_all = asset_input.values\n",
    "\n",
    "    asset_output = pd.DataFrame(new_df, columns=['pm2.5'])\n",
    "    y_all = asset_output.values\n",
    "\n",
    "    # finding out the length of the dataset, and then getting the first 90% as the data i guess\n",
    "    splits = int(0.9 * x_all.shape[0])\n",
    "\n",
    "    timestep = 50\n",
    "\n",
    "    time_offset = time_offset\n",
    "\n",
    "    # in this case, i have to offset the training data by 24 hours, 72, and 168 hours. I will be doing this by adding empty zero rows into the data, and then trimming data off the end so that no invalid data is obtained i guess\n",
    "    # bumping the x data forward by 24 instead of pushing y back by 24 i guess\n",
    "    print(x_all.shape, y_all.shape)\n",
    "    zeros_to_append = np.zeros((time_offset, 5))\n",
    "    x_all = np.concatenate((zeros_to_append, x_all))\n",
    "    zeros_to_append = np.zeros((time_offset, 1))\n",
    "    y_all = np.concatenate((y_all, zeros_to_append))\n",
    "    print(x_all.shape, y_all.shape)\n",
    "\n",
    "    # slicing out the useless zero data at the end and at the start. The array should now be smaller, but should be offsetted the right amount\n",
    "    # idk how the nan values are gonna affect this. Hopefully it does not affect it too much\n",
    "    x_all = x_all[time_offset:-time_offset, :]\n",
    "    y_all = y_all[time_offset:-time_offset, :]\n",
    "    print(x_all.shape, y_all.shape)\n",
    "\n",
    "    x_real = x_all[0:splits, :]\n",
    "    y_real = y_all[0:splits, :]\n",
    "\n",
    "    # testing samples\n",
    "    x_test_real = x_all[splits-timestep:, :]\n",
    "    y_test_real = y_all[splits-timestep:, :]\n",
    "\n",
    "    # scaling the samples accordingly\n",
    "    # most of them fall within a range, and has min and max value, and as such, normalization is choosen i guess\n",
    "    # Normalization\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_x.fit(x_real)\n",
    "    x_scale = scaler_x.transform(x_real)\n",
    "\n",
    "    scaler_y = MinMaxScaler()\n",
    "    scaler_y.fit(y_real)\n",
    "    y_scale = scaler_y.transform(y_real)\n",
    "\n",
    "    # setting up the training and validation dataset\n",
    "    data_generator = TimeseriesGenerator(x_scale, y_scale, length=timestep, sampling_rate=1, batch_size=10000)\n",
    "    x_timestep, y_timestep = data_generator[0]\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_timestep, y_timestep, test_size=0.1, random_state=7567)\n",
    "\n",
    "\n",
    "    # generating the testing data as well i guess\n",
    "    x_test_scale = scaler_x.transform(x_test_real)\n",
    "    y_test_scale = scaler_y.transform(y_test_real)\n",
    "\n",
    "    data_generator = TimeseriesGenerator(x_test_scale, y_test_scale, length=timestep, sampling_rate=1, batch_size=10000)\n",
    "    x_test, y_test = data_generator[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # network section starts here\n",
    "    backend.clear_session()\n",
    "    init_weight = initializers.GlorotUniform(seed=7567)\n",
    "    regu_weight = regularizers.l2(0.001)\n",
    "\n",
    "    backend.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(timestep,5)))\n",
    "    model.add(GRU(100, activation='tanh', recurrent_activation='sigmoid', return_sequences=False, kernel_initializer=init_weight, kernel_regularizer=regu_weight))\n",
    "    model.add(Dense(1, kernel_initializer=init_weight, kernel_regularizer=regu_weight))\n",
    "\n",
    "\n",
    "    my_optmz = optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=my_optmz, loss='mean_squared_error', metrics='mse')\n",
    "    model_hist = model.fit(x_train, y_train, epochs=20, batch_size=64, validation_data=(x_val,y_val), verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # predict the train and validation as well i guess\n",
    "    y_test_pred_scale = model.predict(x_test, verbose=0)\n",
    "    mse_test_scale = mean_squared_error(y_test, y_test_pred_scale)\n",
    "\n",
    "    y_test_pred_real = scaler_y.inverse_transform(y_test_pred_scale)\n",
    "    mse_test_real = mean_squared_error(y_test_real[timestep:,0], y_test_pred_real)\n",
    "\n",
    "\n",
    "    mse_train_list.append(model_hist.history['mse'][-1])\n",
    "    mse_val_list.append(model_hist.history['val_mse'][-1])\n",
    "    # grabbing the scaled mse\n",
    "    mse_test_list.append(mse_test_scale)\n",
    "\n",
    "    print(f'timestep of {timestep}')\n",
    "    print(\"Mean squared error of the scaled testing dataset:\", mse_test_scale)\n",
    "    print(\"Mean squared error of the real testing dataset (USD):\", mse_test_real)\n",
    "\n",
    "print(mse_train_list)\n",
    "print(mse_val_list)\n",
    "print(mse_test_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " # LSTM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# cleaning out the NaNs\n",
    "# this will be loading in the regression dataset\n",
    "# Replace 'your_file.csv' with the path to your CSV file\n",
    "file_path = 'data/Air_pollution.csv'\n",
    "\n",
    "# Use pandas to read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "time_offset_list = [24, 72, 168]\n",
    "mse_train_list = []\n",
    "mse_val_list = []\n",
    "mse_test_list = []\n",
    "\n",
    "for time_offset in time_offset_list:\n",
    "    # taking out the nessesary data\n",
    "    new_df = df[['month', 'day', 'hour', 'pm2.5', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir']]\n",
    "\n",
    "    # dropping out the NaN numbers i guess\n",
    "    new_df = new_df.dropna()\n",
    "\n",
    "    # data preprocessing\n",
    "    # using default values + month, since that was what worked best i guess\n",
    "    asset_input = pd.DataFrame(new_df, columns=['month', 'DEWP', 'TEMP', 'PRES', 'Iws'])\n",
    "    x_all = asset_input.values\n",
    "\n",
    "    asset_output = pd.DataFrame(new_df, columns=['pm2.5'])\n",
    "    y_all = asset_output.values\n",
    "\n",
    "    # finding out the length of the dataset, and then getting the first 90% as the data i guess\n",
    "    splits = int(0.9 * x_all.shape[0])\n",
    "\n",
    "    timestep = 50\n",
    "\n",
    "    time_offset = time_offset\n",
    "\n",
    "    # in this case, i have to offset the training data by 24 hours, 72, and 168 hours. I will be doing this by adding empty zero rows into the data, and then trimming data off the end so that no invalid data is obtained i guess\n",
    "    # bumping the x data forward by 24 instead of pushing y back by 24 i guess\n",
    "    print(x_all.shape, y_all.shape)\n",
    "    zeros_to_append = np.zeros((time_offset, 5))\n",
    "    x_all = np.concatenate((zeros_to_append, x_all))\n",
    "    zeros_to_append = np.zeros((time_offset, 1))\n",
    "    y_all = np.concatenate((y_all, zeros_to_append))\n",
    "    print(x_all.shape, y_all.shape)\n",
    "\n",
    "    # slicing out the useless zero data at the end and at the start. The array should now be smaller, but should be offsetted the right amount\n",
    "    # idk how the nan values are gonna affect this. Hopefully it does not affect it too much\n",
    "    x_all = x_all[time_offset:-time_offset, :]\n",
    "    y_all = y_all[time_offset:-time_offset, :]\n",
    "    print(x_all.shape, y_all.shape)\n",
    "\n",
    "    x_real = x_all[0:splits, :]\n",
    "    y_real = y_all[0:splits, :]\n",
    "\n",
    "    # testing samples\n",
    "    x_test_real = x_all[splits-timestep:, :]\n",
    "    y_test_real = y_all[splits-timestep:, :]\n",
    "\n",
    "    # scaling the samples accordingly\n",
    "    # most of them fall within a range, and has min and max value, and as such, normalization is choosen i guess\n",
    "    # Normalization\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_x.fit(x_real)\n",
    "    x_scale = scaler_x.transform(x_real)\n",
    "\n",
    "    scaler_y = MinMaxScaler()\n",
    "    scaler_y.fit(y_real)\n",
    "    y_scale = scaler_y.transform(y_real)\n",
    "\n",
    "    # setting up the training and validation dataset\n",
    "    data_generator = TimeseriesGenerator(x_scale, y_scale, length=timestep, sampling_rate=1, batch_size=10000)\n",
    "    x_timestep, y_timestep = data_generator[0]\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_timestep, y_timestep, test_size=0.1, random_state=7567)\n",
    "\n",
    "\n",
    "    # generating the testing data as well i guess\n",
    "    x_test_scale = scaler_x.transform(x_test_real)\n",
    "    y_test_scale = scaler_y.transform(y_test_real)\n",
    "\n",
    "    data_generator = TimeseriesGenerator(x_test_scale, y_test_scale, length=timestep, sampling_rate=1, batch_size=10000)\n",
    "    x_test, y_test = data_generator[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # network section starts here\n",
    "    backend.clear_session()\n",
    "    init_weight = initializers.GlorotUniform(seed=7567)\n",
    "    regu_weight = regularizers.l2(0.001)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(timestep,5)))\n",
    "    model.add(LSTM(100, activation='tanh', recurrent_activation='sigmoid', return_sequences=False, kernel_initializer=init_weight, kernel_regularizer=regu_weight))\n",
    "    model.add(Dense(1, kernel_initializer=init_weight, kernel_regularizer=regu_weight))\n",
    "\n",
    "    my_optmz = optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=my_optmz, loss='mean_squared_error', metrics='mse')\n",
    "    model_hist = model.fit(x_train, y_train, epochs=20, batch_size=64, validation_data=(x_val,y_val), verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # predict the train and validation as well i guess\n",
    "    y_test_pred_scale = model.predict(x_test, verbose=0)\n",
    "    mse_test_scale = mean_squared_error(y_test, y_test_pred_scale)\n",
    "\n",
    "    y_test_pred_real = scaler_y.inverse_transform(y_test_pred_scale)\n",
    "    mse_test_real = mean_squared_error(y_test_real[timestep:,0], y_test_pred_real)\n",
    "\n",
    "\n",
    "    mse_train_list.append(model_hist.history['mse'][-1])\n",
    "    mse_val_list.append(model_hist.history['val_mse'][-1])\n",
    "    # grabbing the scaled mse\n",
    "    mse_test_list.append(mse_test_scale)\n",
    "\n",
    "    print(f'timestep of {timestep}')\n",
    "    print(\"Mean squared error of the scaled testing dataset:\", mse_test_scale)\n",
    "    print(\"Mean squared error of the real testing dataset (USD):\", mse_test_real)\n",
    "\n",
    "print(mse_train_list)\n",
    "print(mse_val_list)\n",
    "print(mse_test_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}