{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arlxaCUh5jFl"
   },
   "source": [
    "# setup the model and load in the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gP7CEnsi8Gp4"
   },
   "source": [
    "This part sets up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 335,
     "status": "ok",
     "timestamp": 1692853607960,
     "user": {
      "displayName": "Kang Phan",
      "userId": "17382505543364018191"
     },
     "user_tz": -480
    },
    "id": "gWE4EXKe6aQl"
   },
   "outputs": [],
   "source": [
    "STUDENTID = 567     # this will be used for random states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14487,
     "status": "ok",
     "timestamp": 1692854163379,
     "user": {
      "displayName": "Kang Phan",
      "userId": "17382505543364018191"
     },
     "user_tz": -480
    },
    "id": "B7-VNPmt8E2m",
    "outputId": "1f1050da-e734-4407-d7c8-af006d3d6ecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Number of training examples: 40000\n",
      "Number of validation examples: 10000\n",
      "Number of test examples: 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import logging\n",
    "from utils import *\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL Image to PyTorch Tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the data\n",
    "])\n",
    "\n",
    "# Download and load CIFAR-100 datasets\n",
    "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "\n",
    "# Calculate the sizes for train, validation, and test sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "valid_size = len(train_dataset) - train_size\n",
    "\n",
    "# Split the train dataset into train and validation sets using a random seed\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [train_size, valid_size], generator=torch.Generator().manual_seed(STUDENTID))\n",
    "\n",
    "# Create data loaders for training, validation, and test sets\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(valid_dataset))\n",
    "print(\"Number of test examples:\", len(test_dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec3Yt5GL_ydv"
   },
   "source": [
    "Setting up the train, evaluation, and test functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqEmAt8s8Eiu"
   },
   "source": [
    "Setting up the neural network model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-PaTAXyF6P7P"
   },
   "outputs": [],
   "source": [
    "# creating a model that automatically runs the forward function i guess, since it is easier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# i can set up a parent model, where the forward and train can be overwritten so that i can customize any neural network i want\n",
    "# i can probably setup the train, test, and eval function from somewhere else\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "  def __init__(self, num_classes: int = 100):\n",
    "    super(NeuralNetwork, self).__init__()\n",
    "\n",
    "    # define the layers of the neural network\n",
    "    self.features = nn.Sequential(\n",
    "      #slightly modified version of alexnet to make it smaller\n",
    "\n",
    "      #Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "      nn.Conv2d(3, 64, (3, 3), (4, 4), (2, 2)),\n",
    "      nn.ReLU(True),\n",
    "      nn.BatchNorm2d(64),\n",
    "      # MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "      nn.MaxPool2d((3, 3), (2, 2)),\n",
    "      nn.Conv2d(64, 192, (3, 3), (1, 1), (1, 1)),\n",
    "      nn.ReLU(True),\n",
    "      nn.BatchNorm2d(192),\n",
    "      nn.Conv2d(192, 192, (3, 3), (1, 1), (1, 1)),\n",
    "      nn.ReLU(True),\n",
    "      nn.BatchNorm2d(192),\n",
    "      nn.MaxPool2d((3, 3), (2, 2)),\n",
    "    )\n",
    "\n",
    "    self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "\n",
    "    self.classifier = nn.Sequential(\n",
    "      nn.Dropout(0.5),\n",
    "      # this one i need to recalculate\n",
    "      nn.Linear(6912, 4096),\n",
    "      nn.ReLU(True),\n",
    "      nn.Dropout(0.5),\n",
    "      nn.Linear(4096, 4096),\n",
    "      nn.ReLU(True),\n",
    "      nn.Linear(4096, num_classes),\n",
    "      # it seems like i dont need a softmax activation function here, as it automatically does it with cross entropy loss\n",
    "    )\n",
    "\n",
    "    print('define the self')\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.features(x)\n",
    "    out = self.avgpool(out)\n",
    "    out = torch.flatten(out, 1)   # this one is for the batches, to resize it so that it wont have an issue\n",
    "    out = self.classifier(out)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently at epoch 0 accuracy: tensor(0.0254, device='cuda:0') loss of: 4.530574586486816\n",
      "currently at epoch 1 accuracy: tensor(0.0546, device='cuda:0') loss of: 4.253593740081787\n",
      "currently at epoch 2 accuracy: tensor(0.0811, device='cuda:0') loss of: 4.061206823348999\n",
      "currently at epoch 3 accuracy: tensor(0.1014, device='cuda:0') loss of: 3.900673265838623\n",
      "currently at epoch 4 accuracy: tensor(0.1213, device='cuda:0') loss of: 3.765455725860596\n",
      "currently at epoch 5 accuracy: tensor(0.1402, device='cuda:0') loss of: 3.654823439788818\n",
      "currently at epoch 6 accuracy: tensor(0.1544, device='cuda:0') loss of: 3.5533440673828127\n",
      "currently at epoch 7 accuracy: tensor(0.1710, device='cuda:0') loss of: 3.464891162490845\n",
      "currently at epoch 8 accuracy: tensor(0.1836, device='cuda:0') loss of: 3.380004871749878\n",
      "currently at epoch 9 accuracy: tensor(0.2006, device='cuda:0') loss of: 3.2977186016082762\n",
      "currently at epoch 10 accuracy: tensor(0.2112, device='cuda:0') loss of: 3.2327966472625733\n",
      "currently at epoch 11 accuracy: tensor(0.2225, device='cuda:0') loss of: 3.1635425975799563\n",
      "currently at epoch 12 accuracy: tensor(0.2364, device='cuda:0') loss of: 3.104547842025757\n",
      "currently at epoch 13 accuracy: tensor(0.2452, device='cuda:0') loss of: 3.0411864555358887\n",
      "currently at epoch 14 accuracy: tensor(0.2596, device='cuda:0') loss of: 2.98293957862854\n",
      "currently at epoch 15 accuracy: tensor(0.2689, device='cuda:0') loss of: 2.924830198287964\n",
      "currently at epoch 16 accuracy: tensor(0.2783, device='cuda:0') loss of: 2.866485701751709\n",
      "currently at epoch 17 accuracy: tensor(0.2878, device='cuda:0') loss of: 2.8156428344726563\n",
      "currently at epoch 18 accuracy: tensor(0.2986, device='cuda:0') loss of: 2.762302442932129\n",
      "currently at epoch 19 accuracy: tensor(0.3099, device='cuda:0') loss of: 2.704758491897583\n",
      "currently at epoch 20 accuracy: tensor(0.3176, device='cuda:0') loss of: 2.6562575103759767\n",
      "currently at epoch 21 accuracy: tensor(0.3303, device='cuda:0') loss of: 2.591524119567871\n",
      "currently at epoch 22 accuracy: tensor(0.3401, device='cuda:0') loss of: 2.5458912996292113\n",
      "currently at epoch 23 accuracy: tensor(0.3496, device='cuda:0') loss of: 2.4970140451431275\n",
      "currently at epoch 24 accuracy: tensor(0.3632, device='cuda:0') loss of: 2.4415153841018675\n",
      "currently at epoch 25 accuracy: tensor(0.3717, device='cuda:0') loss of: 2.3917534881591798\n",
      "currently at epoch 26 accuracy: tensor(0.3825, device='cuda:0') loss of: 2.332743692779541\n",
      "currently at epoch 27 accuracy: tensor(0.3952, device='cuda:0') loss of: 2.2781224168777467\n",
      "currently at epoch 28 accuracy: tensor(0.4092, device='cuda:0') loss of: 2.2160930192947386\n",
      "currently at epoch 29 accuracy: tensor(0.4170, device='cuda:0') loss of: 2.164669856452942\n",
      "currently at epoch 30 accuracy: tensor(0.4280, device='cuda:0') loss of: 2.1147302013397216\n",
      "currently at epoch 31 accuracy: tensor(0.4411, device='cuda:0') loss of: 2.049050602531433\n",
      "currently at epoch 32 accuracy: tensor(0.4558, device='cuda:0') loss of: 1.9968428356170653\n",
      "currently at epoch 33 accuracy: tensor(0.4665, device='cuda:0') loss of: 1.9362221141815186\n",
      "currently at epoch 34 accuracy: tensor(0.4784, device='cuda:0') loss of: 1.88321175365448\n",
      "currently at epoch 35 accuracy: tensor(0.4932, device='cuda:0') loss of: 1.8190174150466918\n",
      "currently at epoch 36 accuracy: tensor(0.5038, device='cuda:0') loss of: 1.7638589466094972\n",
      "currently at epoch 37 accuracy: tensor(0.5164, device='cuda:0') loss of: 1.7051085578918457\n",
      "currently at epoch 38 accuracy: tensor(0.5335, device='cuda:0') loss of: 1.6433991706848146\n",
      "currently at epoch 39 accuracy: tensor(0.5494, device='cuda:0') loss of: 1.5772817876815797\n",
      "currently at epoch 40 accuracy: tensor(0.5581, device='cuda:0') loss of: 1.5322750385284425\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [8], line 93\u001B[0m\n\u001B[0;32m     79\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m argDict\n\u001B[0;32m     83\u001B[0m argDict \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     84\u001B[0m   \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlogger\u001B[39m\u001B[38;5;124m'\u001B[39m: logger,\n\u001B[0;32m     85\u001B[0m   \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0.001\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     90\u001B[0m   \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcriterion\u001B[39m\u001B[38;5;124m'\u001B[39m: nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[0;32m     91\u001B[0m }\n\u001B[1;32m---> 93\u001B[0m temp \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margDict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn [8], line 32\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, argDict, givenDataloader)\u001B[0m\n\u001B[0;32m     29\u001B[0m accuracy_values \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     30\u001B[0m loss_values \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m---> 32\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx, (data, label) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(givenDataloader):\n\u001B[0;32m     33\u001B[0m   data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     34\u001B[0m   label \u001B[38;5;241m=\u001B[39m label\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[1;32mD:\\gitprojects\\FinerObjectDetectionVENV\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mD:\\gitprojects\\FinerObjectDetectionVENV\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mD:\\gitprojects\\FinerObjectDetectionVENV\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mD:\\gitprojects\\FinerObjectDetectionVENV\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mD:\\gitprojects\\FinerObjectDetectionVENV\\lib\\site-packages\\torch\\utils\\data\\dataset.py:295\u001B[0m, in \u001B[0;36mSubset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m    293\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m    294\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m idx]]\n\u001B[1;32m--> 295\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n",
      "File \u001B[1;32mD:\\gitprojects\\FinerObjectDetectionVENV\\lib\\site-packages\\torchvision\\datasets\\cifar.py:118\u001B[0m, in \u001B[0;36mCIFAR10.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m    115\u001B[0m img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(img)\n\u001B[0;32m    117\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 118\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    121\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform(target)\n",
      "File \u001B[1;32mD:\\gitprojects\\FinerObjectDetectionVENV\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32mD:\\gitprojects\\FinerObjectDetectionVENV\\lib\\site-packages\\torchvision\\transforms\\transforms.py:135\u001B[0m, in \u001B[0;36mToTensor.__call__\u001B[1;34m(self, pic)\u001B[0m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[0;32m    128\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    133\u001B[0m \u001B[38;5;124;03m        Tensor: Converted image.\u001B[39;00m\n\u001B[0;32m    134\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 135\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\gitprojects\\FinerObjectDetectionVENV\\lib\\site-packages\\torchvision\\transforms\\functional.py:167\u001B[0m, in \u001B[0;36mto_tensor\u001B[1;34m(pic)\u001B[0m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pic\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    166\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m255\u001B[39m \u001B[38;5;241m*\u001B[39m img\n\u001B[1;32m--> 167\u001B[0m img \u001B[38;5;241m=\u001B[39m \u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpic\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetbands\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    168\u001B[0m \u001B[38;5;66;03m# put it from HWC to CHW format\u001B[39;00m\n\u001B[0;32m    169\u001B[0m img \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39mpermute((\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m.\u001B[39mcontiguous()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# i would likely make it so that to use the training and evaluation function, i will just pass the arguments, network, and the logger into the function and they will train it automatically for me, seems like an easier implementation anyways compared to trying to shove it into the class itself i guess\n",
    "from utils import *\n",
    "\n",
    "def train(model, argDict, givenDataloader):\n",
    "  # get all the stuff out\n",
    "  # update the learning rate of the optimizer\n",
    "  for param_group in argDict['optimizer'].param_groups:\n",
    "    param_group['lr'] = argDict['lr']\n",
    "\n",
    "  # get the device type, and set it to cuda\n",
    "  if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "  else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "  # casting model to device\n",
    "  model.to(device)\n",
    "\n",
    "  # training for multiple epochs\n",
    "  epoch_accuracy_values = []\n",
    "  epoch_loss_values = []\n",
    "\n",
    "  best_epoch_value = 0\n",
    "  best_epoch_epoch = 0\n",
    "\n",
    "  for currentEpoch in range(argDict['maxEpoch']):\n",
    "    accuracy_values = []\n",
    "    loss_values = []\n",
    "\n",
    "    for idx, (data, label) in enumerate(givenDataloader):\n",
    "      data = data.to(device)\n",
    "      label = label.to(device)\n",
    "\n",
    "      # this will be the training loop\n",
    "      outputs = model(data)\n",
    "\n",
    "      loss = argDict['criterion'](outputs, label)\n",
    "\n",
    "      # backward pass and optimization\n",
    "      argDict['optimizer'].zero_grad()\n",
    "      loss.backward()\n",
    "      argDict['optimizer'].step()\n",
    "\n",
    "      # data logging phase, obtains loss and accuracy\n",
    "      loss_values.append((loss.item()))\n",
    "\n",
    "      # getting the accuracy\n",
    "      _, predicted = torch.max(outputs, 1)\n",
    "      accuracy = (predicted == label).float().mean()\n",
    "      accuracy_values.append(accuracy)\n",
    "\n",
    "    # calculating epoch losses\n",
    "    epoch_loss = np.mean(loss_values)\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    epoch_accuracy = torch.mean(torch.stack(accuracy_values))   # due to it being tensor\n",
    "    epoch_accuracy_values.append(epoch_accuracy)\n",
    "\n",
    "    tempString = 'currently at epoch ' + str(currentEpoch) + ' accuracy: ' + str(epoch_accuracy) + ' loss of: ' + str(epoch_loss)\n",
    "    argDict['logger'].log(tempString)\n",
    "\n",
    "    # evaluating whether to break training or not\n",
    "    if epoch_accuracy > best_epoch_value:\n",
    "      best_epoch_value = epoch_accuracy\n",
    "      best_epoch_epoch = currentEpoch\n",
    "\n",
    "      # save the model as well\n",
    "      save_model_to_file(model, argDict['outputName'], argDict['outputName'])\n",
    "    else:\n",
    "      if (currentEpoch - best_epoch_epoch) > argDict['idleEpoch']:\n",
    "        # this means that this is the max trained  epoch\n",
    "        break\n",
    "\n",
    "  argDict['epoch_loss_values'] = epoch_loss_values\n",
    "  argDict['epoch_accuracy_values'] = epoch_accuracy_values\n",
    "  argDict['trainingStopEpoch'] = currentEpoch\n",
    "\n",
    "  # saves the dictionary as well\n",
    "  return argDict\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "\n",
    "argDict = {\n",
    "  'logger': MyLogger(\"FirstModelTest.log\"),\n",
    "  'lr': 0.001,\n",
    "  'maxEpoch': 250,\n",
    "  'idleEpoch': 25,\n",
    "  'outputName': 'FirstModelTest',\n",
    "  'optimizer': optim.SGD(model.parameters(), lr=0.001),\n",
    "  'criterion': nn.CrossEntropyLoss()\n",
    "}\n",
    "\n",
    "temp = train(model, argDict, train_loader)\n",
    "save_dict_to_file(str(temp), argDict['outputName'], argDict['outputName'])\n",
    "\n",
    "del model\n",
    "del argDict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "\n",
    "argDict = {\n",
    "  'logger': MyLogger(\"higherLearningRate.log\"),\n",
    "  'lr': 0.003,\n",
    "  'maxEpoch': 250,\n",
    "  'idleEpoch': 25,\n",
    "  'outputName': 'higherLearningRate',\n",
    "  'optimizer': optim.SGD(model.parameters(), lr=0.001),\n",
    "  'criterion': nn.CrossEntropyLoss()\n",
    "}\n",
    "\n",
    "temp = train(model, argDict, train_loader)\n",
    "save_dict_to_file(str(temp), argDict['outputName'], argDict['outputName'])\n",
    "\n",
    "del model\n",
    "del argDict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "\n",
    "argDict = {\n",
    "  'logger': MyLogger(\"lowerLearningRate.log\"),\n",
    "  'lr': 0.0005,\n",
    "  'maxEpoch': 250,\n",
    "  'idleEpoch': 25,\n",
    "  'outputName': 'lowerLearningRate',\n",
    "  'optimizer': optim.SGD(model.parameters(), lr=0.001),\n",
    "  'criterion': nn.CrossEntropyLoss()\n",
    "}\n",
    "\n",
    "temp = train(model, argDict, train_loader)\n",
    "save_dict_to_file(str(temp), argDict['outputName'], argDict['outputName'])\n",
    "\n",
    "del model\n",
    "del argDict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def save_dict_to_file(dict, folder_path, filename):\n",
    "  import json\n",
    "  import os\n",
    "  def is_json_serializable(obj):\n",
    "    try:\n",
    "      json.dumps(obj)\n",
    "      return True\n",
    "    except:\n",
    "      return False\n",
    "\n",
    "  def copy_dict_with_serializable_items(original_dict):\n",
    "    new_dict = {}\n",
    "    for key, value in original_dict.items():\n",
    "      if is_json_serializable(value):\n",
    "        new_dict[key] = value\n",
    "    return new_dict\n",
    "\n",
    "  # Check if the folder exists, and create it if not\n",
    "  if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "  # Add the \".pth\" extension to the filename if missing\n",
    "  if not filename.endswith(\".json\"):\n",
    "    filename += \".json\"\n",
    "\n",
    "  # modify the .json file so that it can be saved to file\n",
    "  new_dict = copy_dict_with_serializable_items(dict)\n",
    "\n",
    "  # Save the dictionary to a JSON file\n",
    "  filename = os.path.join(folder_path, filename)\n",
    "  with open(filename, \"w\") as json_file:\n",
    "    json.dump(new_dict, json_file)\n",
    "\n",
    "save_dict_to_file(argDict, argDict['outputName'], argDict['outputName'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMPhCe+CPqr8788BodV/T4I",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}